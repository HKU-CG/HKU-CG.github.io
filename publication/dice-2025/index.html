<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 4.8.0 for Hugo"><meta name=author content="Computer Graphics and Visualization Lab"><meta name=description content="Reconstructing 3D hand-face interactions with deformations from a single image is a challenging yet crucial task with broad applications in AR, VR, and gaming. The challenges stem from self-occlusions during single-view hand-face interactions, diverse spatial relationships between hands and face, complex deformations, and the ambiguity of the single-view setting. The first and only method for hand-face interaction recovery, Decaf, introduces a global fitting optimization guided by contact and deformation estimation networks trained on studio-collected data with 3D annotations. However, Decaf suffers from a time-consuming optimization process and limited generalization capability due to its reliance on 3D annotations of hand-face interaction data. To address these issues, we present DICE, the first end-to-end method for Deformation-aware hand-face Interaction reCovEry from a single image. DICE estimates the poses of hands and faces, contacts, and deformations simultaneously using a Transformer-based architecture. It features disentangling the regression of local deformation fields and global mesh vertex locations into two network branches, enhancing deformation and contact estimation for precise and robust hand-face mesh recovery. To improve generalizability, we propose a weakly-supervised training approach that augments the training set using in-the-wild images without 3D ground-truth annotations, employing the depths of 2D keypoints estimated by off-the-shelf models and adversarial priors of poses for supervision. Our experiments demonstrate that DICE achieves state-of-the-art performance on a standard benchmark and in-the-wild data in terms of accuracy and physical plausibility. Additionally, our method operates at an interactive rate (20 fps) on an Nvidia 4090 GPU, whereas Decaf requires more than 15 seconds for a single image. Our code will be publicly available upon publication."><link rel=alternate hreflang=en-us href=/publication/dice-2025/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#2962ff"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin=anonymous async></script><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap"><link rel=stylesheet href=/css/wowchemy.css><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/images/icon_hu2e4579b938cb71ab526c5021d9215e0c_79015_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/images/icon_hu2e4579b938cb71ab526c5021d9215e0c_79015_192x192_fill_lanczos_center_2.png><link rel=canonical href=/publication/dice-2025/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="HKU CGVU Lab"><meta property="og:url" content="/publication/dice-2025/"><meta property="og:title" content="DICE: End-to-end Deformation Capture of Hand-Face Interactions from a Single Image | HKU CGVU Lab"><meta property="og:description" content="Reconstructing 3D hand-face interactions with deformations from a single image is a challenging yet crucial task with broad applications in AR, VR, and gaming. The challenges stem from self-occlusions during single-view hand-face interactions, diverse spatial relationships between hands and face, complex deformations, and the ambiguity of the single-view setting. The first and only method for hand-face interaction recovery, Decaf, introduces a global fitting optimization guided by contact and deformation estimation networks trained on studio-collected data with 3D annotations. However, Decaf suffers from a time-consuming optimization process and limited generalization capability due to its reliance on 3D annotations of hand-face interaction data. To address these issues, we present DICE, the first end-to-end method for Deformation-aware hand-face Interaction reCovEry from a single image. DICE estimates the poses of hands and faces, contacts, and deformations simultaneously using a Transformer-based architecture. It features disentangling the regression of local deformation fields and global mesh vertex locations into two network branches, enhancing deformation and contact estimation for precise and robust hand-face mesh recovery. To improve generalizability, we propose a weakly-supervised training approach that augments the training set using in-the-wild images without 3D ground-truth annotations, employing the depths of 2D keypoints estimated by off-the-shelf models and adversarial priors of poses for supervision. Our experiments demonstrate that DICE achieves state-of-the-art performance on a standard benchmark and in-the-wild data in terms of accuracy and physical plausibility. Additionally, our method operates at an interactive rate (20 fps) on an Nvidia 4090 GPU, whereas Decaf requires more than 15 seconds for a single image. Our code will be publicly available upon publication."><meta property="og:image" content="/publication/dice-2025/featured.png"><meta property="twitter:image" content="/publication/dice-2025/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2025-01-01T00:00:00+00:00"><meta property="article:modified_time" content="2025-01-01T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"/publication/dice-2025/"},"headline":"DICE: End-to-end Deformation Capture of Hand-Face Interactions from a Single Image","image":["/publication/dice-2025/featured.png"],"datePublished":"2025-01-01T00:00:00Z","dateModified":"2025-01-01T00:00:00Z","author":{"@type":"Person","name":"Qingxuan Wu"},"publisher":{"@type":"Organization","name":"HKU CGVU Lab","logo":{"@type":"ImageObject","url":"/images/logo_hu2e4579b938cb71ab526c5021d9215e0c_79015_192x192_fit_lanczos_2.png"}},"description":"Reconstructing 3D hand-face interactions with deformations from a single image is a challenging yet crucial task with broad applications in AR, VR, and gaming. The challenges stem from self-occlusions during single-view hand-face interactions, diverse spatial relationships between hands and face, complex deformations, and the ambiguity of the single-view setting. The first and only method for hand-face interaction recovery, Decaf, introduces a global fitting optimization guided by contact and deformation estimation networks trained on studio-collected data with 3D annotations. However, Decaf suffers from a time-consuming optimization process and limited generalization capability due to its reliance on 3D annotations of hand-face interaction data. To address these issues, we present DICE, the first end-to-end method for Deformation-aware hand-face Interaction reCovEry from a single image. DICE estimates the poses of hands and faces, contacts, and deformations simultaneously using a Transformer-based architecture. It features disentangling the regression of local deformation fields and global mesh vertex locations into two network branches, enhancing deformation and contact estimation for precise and robust hand-face mesh recovery. To improve generalizability, we propose a weakly-supervised training approach that augments the training set using in-the-wild images without 3D ground-truth annotations, employing the depths of 2D keypoints estimated by off-the-shelf models and adversarial priors of poses for supervision. Our experiments demonstrate that DICE achieves state-of-the-art performance on a standard benchmark and in-the-wild data in terms of accuracy and physical plausibility. Additionally, our method operates at an interactive rate (20 fps) on an Nvidia 4090 GPU, whereas Decaf requires more than 15 seconds for a single image. Our code will be publicly available upon publication."}</script><title>DICE: End-to-end Deformation Capture of Hand-Face Interactions from a Single Image | HKU CGVU Lab</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><script>window.wcDarkLightEnabled=true;</script><script>const isSiteThemeDark=false;</script><script src=/js/load-theme.js></script><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/><img src=/images/logo_hu2e4579b938cb71ab526c5021d9215e0c_79015_0x70_resize_lanczos_2.png alt="HKU CGVU Lab"></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/><img src=/images/logo_hu2e4579b938cb71ab526c5021d9215e0c_79015_0x70_resize_lanczos_2.png alt="HKU CGVU Lab"></a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/people><span>People</span></a></li><li class=nav-item><a class="nav-link active" href=/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/courses><span>Teaching</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav><div class=pub><div class="article-container pt-3"><h1>DICE: End-to-end Deformation Capture of Hand-Face Interactions from a Single Image</h1><div class=article-metadata><div><span><a href=/author/qingxuan-wu/>Qingxuan Wu</a></span>, <span><a href=/author/zhiyang-dou/>Zhiyang Dou</a></span>, <span><a href=/author/sirui-xu/>Sirui Xu</a></span>, <span><a href=/author/soshi-shimada/>Soshi Shimada</a></span>, <span><a href=/author/chen-wang/>Chen Wang</a></span>, <span><a href=/author/zhengming-yu/>Zhengming Yu</a></span>, <span><a href=/author/yuan-liu/>Yuan Liu</a></span>, <span><a href=/author/cheng-lin/>Cheng Lin</a></span>, <span><a href=/author/zeyu-cao/>Zeyu Cao</a></span>, <span><a href=/author/taku-komura/>Taku Komura</a></span>, <span><a href=/author/vladislav-golyanik/>Vladislav Golyanik</a></span>, <span><a href=/author/christian-theobalt/>Christian Theobalt</a></span>, <span><a href=/author/wenping-wang/>Wenping Wang</a></span>, <span><a href=/author/lingjie-liu/>Lingjie Liu</a></span></div><span class=article-date>January 2025</span></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:388px><div style=position:relative><img src=/publication/dice-2025/featured_hu5114f6f136f898fd74f0eacf05cf8e1d_1512188_720x0_resize_lanczos_2.png alt class=featured-image></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>Reconstructing 3D hand-face interactions with deformations from a single image is a challenging yet crucial task with broad applications in AR, VR, and gaming. The challenges stem from self-occlusions during single-view hand-face interactions, diverse spatial relationships between hands and face, complex deformations, and the ambiguity of the single-view setting. The first and only method for hand-face interaction recovery, Decaf, introduces a global fitting optimization guided by contact and deformation estimation networks trained on studio-collected data with 3D annotations. However, Decaf suffers from a time-consuming optimization process and limited generalization capability due to its reliance on 3D annotations of hand-face interaction data. To address these issues, we present DICE, the first end-to-end method for Deformation-aware hand-face Interaction reCovEry from a single image. DICE estimates the poses of hands and faces, contacts, and deformations simultaneously using a Transformer-based architecture. It features disentangling the regression of local deformation fields and global mesh vertex locations into two network branches, enhancing deformation and contact estimation for precise and robust hand-face mesh recovery. To improve generalizability, we propose a weakly-supervised training approach that augments the training set using in-the-wild images without 3D ground-truth annotations, employing the depths of 2D keypoints estimated by off-the-shelf models and adversarial priors of poses for supervision. Our experiments demonstrate that DICE achieves state-of-the-art performance on a standard benchmark and in-the-wild data in terms of accuracy and physical plausibility. Additionally, our method operates at an interactive rate (20 fps) on an Nvidia 4090 GPU, whereas Decaf requires more than 15 seconds for a single image. Our code will be publicly available upon publication.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#1>Conference paper</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">ICLR 2025</div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=article-tags><a class="badge badge-light" href=/tag/hand-and-face-mesh-reconstruction/>hand and face mesh reconstruction</a>
<a class="badge badge-light" href=/tag/interaction-reconstruction/>interaction reconstruction</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=/publication/dice-2025/&text=DICE:%20End-to-end%20Deformation%20Capture%20of%20Hand-Face%20Interactions%20from%20a%20Single%20Image" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=/publication/dice-2025/&t=DICE:%20End-to-end%20Deformation%20Capture%20of%20Hand-Face%20Interactions%20from%20a%20Single%20Image" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=DICE:%20End-to-end%20Deformation%20Capture%20of%20Hand-Face%20Interactions%20from%20a%20Single%20Image&body=/publication/dice-2025/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=/publication/dice-2025/&title=DICE:%20End-to-end%20Deformation%20Capture%20of%20Hand-Face%20Interactions%20from%20a%20Single%20Image" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=DICE:%20End-to-end%20Deformation%20Capture%20of%20Hand-Face%20Interactions%20from%20a%20Single%20Image%20/publication/dice-2025/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=/publication/dice-2025/&title=DICE:%20End-to-end%20Deformation%20Capture%20of%20Hand-Face%20Interactions%20from%20a%20Single%20Image" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=/author/zhiyang-dou/><img class="avatar mr-3 avatar-circle" src=/author/zhiyang-dou/avatar_huabca85c93043588a4748def6dc5b0774_41772_270x270_fill_q90_lanczos_center.jpg alt="Zhiyang Dou"></a><div class=media-body><h5 class=card-title><a href=/author/zhiyang-dou/>Zhiyang Dou</a></h5><h6 class=card-subtitle>PhD, since Aug. 2020.<br>Co-supv. by Prof. Wenping Wang.</h6><ul class=network-icon aria-hidden=true><li><a href="https://scholar.google.com/citations?user=SLRYlKsAAAAJ&hl=en" target=_blank rel=noopener><i class="fas fa-user-graduate"></i></a></li><li><a href=https://github.com/Frank-ZY-Dou target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.colorado.edu/cs/academics/ target=_blank rel=noopener><i class="Custom_University Custom_University-university-logo"></i></a></li></ul></div></div><div class="media author-card content-widget-hr"><a href=/author/taku-komura/><img class="avatar mr-3 avatar-circle" src=/author/taku-komura/avatar_hu13dcf3a41fad6ceb125ba13611db0352_110674_270x270_fill_q90_lanczos_center.jpg alt="Taku Komura"></a><div class=media-body><h5 class=card-title><a href=/author/taku-komura/>Taku Komura</a></h5><h6 class=card-subtitle>Professor</h6><ul class=network-icon aria-hidden=true><li><a href=mailto:taku@cs.hku.hk><i class="fas fa-envelope"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=/publication/surfd-2024/>Surf-D: Generating High-Quality Surfaces of Arbitrary Topologies Using Diffusion Models</a></li><li><a href=/publication/emdm-2024/>EMDM: Efficient Motion Diffusion Model for Fast and High-Quality Motion Generation</a></li><li><a href=/publication/tore-2023/>TORE: Token Reduction for Efficient Human Mesh Recovery with Transformer</a></li><li><a href=/publication/cat++-2024/>Coverage Axis++: Efficient Skeletal Points Selection for 3D Shape Skeletonization</a></li><li><a href=/publication/cbil-2024/>CBIL: Collective Behavior Imitation Learning for Fish from Real Videos</a></li></ul></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js></script><script async defer src="https://maps.googleapis.com/maps/api/js?key=AIzaSyDRIRPRmqFRMGZ0_d60XJceM3MMWhGS1NQ%20%20"></script><script src=https://cdnjs.cloudflare.com/ajax/libs/gmaps.js/0.4.25/gmaps.min.js integrity="sha256-7vjlAeb8OaTrCXZkCNun9djzuB2owUsaO72kXaFDBJs=" crossorigin=anonymous></script><script>const code_highlighting=true;</script><script>const search_config={"indexURI":"/index.json","minLength":1,"threshold":0.3};const i18n={"no_results":"No results found","placeholder":"Search...","results":"results found"};const content_type={'post':"Posts",'project':"Projects",'publication':"Publications",'talk':"Talks",'slides':"Slides"};</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/wowchemy.min.4c2bca31150ce93c5a5e43b8a50f22fd.js></script><div class=container><footer class=site-footer><p class=powered-by></p><p class=powered-by>Published with
<a href=https://wowchemy.com target=_blank rel=noopener>Wowchemy</a> â€”
the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.
<span class=float-right aria-hidden=true><a href=# class=back-to-top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body></html>