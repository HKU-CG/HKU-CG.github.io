[{"authors":["Sebastian Starke"],"categories":null,"content":"","date":1688169600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1610746490,"objectID":"7ad9e0d25607c23558dc1a84633335df","permalink":"/author/sebastian-starke/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sebastian-starke/","section":"authors","summary":"","tags":null,"title":"Sebastian Starke","type":"authors"},{"authors":["He Zhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2aee2d6999883d43b6c680341589a323","permalink":"/author/he-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/he-zhang/","section":"authors","summary":"","tags":null,"title":"He Zhang","type":"authors"},{"authors":["Daniel Holden"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"082ebc645a06938f078841988813e0b3","permalink":"/author/daniel-holden/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/daniel-holden/","section":"authors","summary":"","tags":null,"title":"Daniel Holden","type":"authors"},{"authors":["He Wang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d097355d4439a4fb990040fcdb2fcd47","permalink":"/author/he-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/he-wang/","section":"authors","summary":"","tags":null,"title":"He Wang","type":"authors"},{"authors":["Taku Komura"],"categories":null,"content":"Taku Komura joined Hong Kong University in 2020. Before joining HKU, he worked at the University of Edinburgh (2006-2020), City University of Hong Kong (2002-2006) and RIKEN (2000-2002). He received his BSc, MSc and PhD in Information Science from University of Tokyo. His research has focused on data-driven character animation, physically-based character animation, crowd simulation, 3D modelling, cloth animation, anatomy-based modelling and robotics. Recently, his main research interests have been on physically-based animation and the application of machine learning techniques for animation synthesis. He received the Royal Society Industry Fellowship (2014) and the Google AR/VR Research Award (2017).\n","date":1740787200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1740787200,"objectID":"8a8179ec372f743fdf1427e6a240d566","permalink":"/author/taku-komura/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/taku-komura/","section":"authors","summary":"Taku Komura joined Hong Kong University in 2020. Before joining HKU, he worked at the University of Edinburgh (2006-2020), City University of Hong Kong (2002-2006) and RIKEN (2000-2002). He received his BSc, MSc and PhD in Information Science from University of Tokyo.","tags":null,"title":"Taku Komura","type":"authors"},{"authors":["Linxu Fan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"18073cc797214f671971bde290209d75","permalink":"/author/linxu-fan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/linxu-fan/","section":"authors","summary":"","tags":null,"title":"Linxu Fan","type":"authors"},{"authors":["Zhiyang Dou"],"categories":null,"content":"Personal Page\n","date":1740787200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1740787200,"objectID":"fe143528c14e839e0d8b9ebbf89b532f","permalink":"/author/zhiyang-dou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhiyang-dou/","section":"authors","summary":"Personal Page","tags":null,"title":"Zhiyang Dou","type":"authors"},{"authors":["Dafei Qin"],"categories":null,"content":"Personal Page\n","date":1691366400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1610746490,"objectID":"1aa7d3cea491498164f398fa0779fe41","permalink":"/author/dafei-qin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dafei-qin/","section":"authors","summary":"Personal Page","tags":null,"title":"Dafei Qin","type":"authors"},{"authors":["Mingyi Shi"],"categories":null,"content":"Personal Page\n","date":1719792000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1625175290,"objectID":"5421a248da7089f5dada17d1398133f0","permalink":"/author/mingyi-shi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mingyi-shi/","section":"authors","summary":"Personal Page","tags":null,"title":"Mingyi Shi","type":"authors"},{"authors":["Jintao Lu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ba9b9175e4243340aea23b275623cbb0","permalink":"/author/jintao-lu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jintao-lu/","section":"authors","summary":"","tags":null,"title":"Jintao Lu","type":"authors"},{"authors":["Huancheng Lin"],"categories":null,"content":"","date":1723161600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1610746490,"objectID":"5a9d6cb1312e8c1f9aa8f5e28b5ee350","permalink":"/author/huancheng-lin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/huancheng-lin/","section":"authors","summary":"","tags":null,"title":"Huancheng Lin","type":"authors"},{"authors":["Kemeng Huang"],"categories":null,"content":"Personal Page\n","date":1711238400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1610746490,"objectID":"1898cca49b936a5199196bbbfc2608a1","permalink":"/author/kemeng-huang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kemeng-huang/","section":"authors","summary":"Personal Page","tags":null,"title":"Kemeng Huang","type":"authors"},{"authors":["Guying Lin"],"categories":null,"content":"Personal Page\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bcf9ac8287f306fb16182063426ba145","permalink":"/author/guying-lin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/guying-lin/","section":"authors","summary":"Personal Page","tags":null,"title":"Guying Lin","type":"authors"},{"authors":["Floyd M. Chitalu"],"categories":null,"content":"Personal Page\n","date":1723161600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1610746490,"objectID":"bce7cd96f28d6ec6f5e9d066d4110ec1","permalink":"/author/floyd-m.-chitalu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/floyd-m.-chitalu/","section":"authors","summary":"Personal Page","tags":null,"title":"Floyd M. Chitalu","type":"authors"},{"authors":["Wenjia Wang"],"categories":null,"content":"Personal Page\n","date":1740787200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1740787200,"objectID":"41842baf02e3ce48e3eb089801f039ad","permalink":"/author/wenjia-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/wenjia-wang/","section":"authors","summary":"Personal Page","tags":null,"title":"Wenjia Wang","type":"authors"},{"authors":["Zhouyingcheng Liao"],"categories":null,"content":"Personal Homepage: https://zycliao.com\n","date":1721606400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1723066490,"objectID":"4603e99a6aac4b5d4717afb9c7c7a2be","permalink":"/author/zhouyingcheng-liao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhouyingcheng-liao/","section":"authors","summary":"Personal Homepage: https://zycliao.com","tags":null,"title":"Zhouyingcheng Liao","type":"authors"},{"authors":["Leo Ho"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"445fd2c5529f8406700507ff67e9f2e4","permalink":"/author/leo-ho/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/leo-ho/","section":"authors","summary":"","tags":null,"title":"Leo Ho","type":"authors"},{"authors":["Yinghao Huang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d74122d5cb5163e5c6d2d00b9ddb73bc","permalink":"/author/yinghao-huang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yinghao-huang/","section":"authors","summary":"","tags":null,"title":"Yinghao Huang","type":"authors"},{"authors":["Yuke Lou"],"categories":null,"content":"Personal Page\n","date":1733184000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1733261690,"objectID":"c6439a61c2c176732ddecf49d6650bae","permalink":"/author/yuke-lou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yuke-lou/","section":"authors","summary":"Personal Page","tags":null,"title":"Yuke Lou","type":"authors"},{"authors":["Chen Peng"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"494d3c2afb9025165e04412878659973","permalink":"/author/chen-peng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chen-peng/","section":"authors","summary":"","tags":null,"title":"Chen Peng","type":"authors"},{"authors":["Xiaohan Ye"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1ca89c3749f759061d0c07fb789b40dd","permalink":"/author/xiaohan-ye/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiaohan-ye/","section":"authors","summary":"","tags":null,"title":"Xiaohan Ye","type":"authors"},{"authors":["Xinyu Lu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"663fe92c622fd16c8a646f0795be2e4c","permalink":"/author/xinyu-lu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xinyu-lu/","section":"authors","summary":"","tags":null,"title":"Xinyu Lu","type":"authors"},{"authors":["admin"],"categories":null,"content":"CGVU Lab, led by Prof. Taku Komura, belongs to the Department of Computer Science, the University of Hong Kong. Our research focus is on physically-based animation and the application of machine learning techniques for animation synthesis.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/computer-graphics-and-visualization-lab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/computer-graphics-and-visualization-lab/","section":"authors","summary":"CGVU Lab, led by Prof. Taku Komura, belongs to the Department of Computer Science, the University of Hong Kong. Our research focus is on physically-based animation and the application of machine learning techniques for animation synthesis.","tags":null,"title":"Computer Graphics and Visualization Lab","type":"authors"},{"authors":["Liang Pan","Zeshi Yang","Zhiyang Dou","Wenjia Wang","Buzhen Huang","Bo Dai","Taku Komura","Jingbo Wang"],"categories":[],"content":"","date":1740787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1740787200,"objectID":"374300899ccdb62a301d09f486a4680e","permalink":"/publication/tokenhsi-2025/","publishdate":"2025-03-01T00:00:00Z","relpermalink":"/publication/tokenhsi-2025/","section":"publication","summary":"The synthesis of realistic and physically plausible human-scene interaction animations presents a critical and complex challenge in computer vision and embodied AI. Recent advances primarily focus on developing specialized character controllers for individual interaction tasks, such as contacting and carrying, often overlooking the need to establish a unified policy for versatile skills. This limitation hinders the ability to generate high-quality motions across a variety of challenging human-scene interaction tasks that require the integration of multiple skills, e.g., walking to a chair and sitting down while carrying a box. To address this issue, we present TokenHSI, a unified controller designed to synthesize various types of human-scene interaction animations. The key innovation of our framework is the use of tokenized proprioception for the simulated character, combined with various task observations, complemented by a masking mechanism that enables the selection of tasks on demand. In addition, our unified policy network is equipped with flexible input size capabilities, enabling efficient adaptation of learned foundational skills to new environments and tasks. By introducing additional input tokens to the pre-trained policy, we can not only modify interaction targets but also integrate learned skills to address diverse challenges. Overall, our framework facilitates the generation of a wide range of character animations, significantly improving flexibility and adaptability in human-scene interactions.","tags":["Physics-based Character Animation","RL"],"title":"TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization","type":"publication"},{"authors":["Qingxuan Wu","Zhiyang Dou","Sirui Xu","Soshi Shimada","Chen Wang","Zhengming Yu","Yuan Liu","Cheng Lin","Zeyu Cao","Taku Komura","Vladislav Golyanik","Christian Theobalt","Wenping Wang","Lingjie Liu"],"categories":[],"content":"","date":1735689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1735689600,"objectID":"901ff29a8edf78d86b33805c712fabfe","permalink":"/publication/dice-2025/","publishdate":"2025-01-01T00:00:00Z","relpermalink":"/publication/dice-2025/","section":"publication","summary":"Reconstructing 3D hand-face interactions with deformations from a single image is a challenging yet crucial task with broad applications in AR, VR, and gaming. The challenges stem from self-occlusions during single-view hand-face interactions, diverse spatial relationships between hands and face, complex deformations, and the ambiguity of the single-view setting. The first and only method for hand-face interaction recovery, Decaf, introduces a global fitting optimization guided by contact and deformation estimation networks trained on studio-collected data with 3D annotations. However, Decaf suffers from a time-consuming optimization process and limited generalization capability due to its reliance on 3D annotations of hand-face interaction data. To address these issues, we present DICE, the first end-to-end method for Deformation-aware hand-face Interaction reCovEry from a single image. DICE estimates the poses of hands and faces, contacts, and deformations simultaneously using a Transformer-based architecture. It features disentangling the regression of local deformation fields and global mesh vertex locations into two network branches, enhancing deformation and contact estimation for precise and robust hand-face mesh recovery. To improve generalizability, we propose a weakly-supervised training approach that augments the training set using in-the-wild images without 3D ground-truth annotations, employing the depths of 2D keypoints estimated by off-the-shelf models and adversarial priors of poses for supervision. Our experiments demonstrate that DICE achieves state-of-the-art performance on a standard benchmark and in-the-wild data in terms of accuracy and physical plausibility. Additionally, our method operates at an interactive rate (20 fps) on an Nvidia 4090 GPU, whereas Decaf requires more than 15 seconds for a single image. Our code will be publicly available upon publication.","tags":["hand and face mesh reconstruction","interaction reconstruction"],"title":"DICE: End-to-end Deformation Capture of Hand-Face Interactions from a Single Image","type":"publication"},{"authors":null,"categories":[],"content":"DICE was accepted to ICLR 2025. Project Page: https://frank-zy-dou.github.io/projects/DICE/index.html.\n","date":1735689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1735689600,"objectID":"6337ab367924f5d5c40c2eb5cf29ea79","permalink":"/talk/202407_siggraph/","publishdate":"2025-01-01T00:00:00Z","relpermalink":"/talk/202407_siggraph/","section":"talk","summary":"Check our new paper: DICE.","tags":null,"title":"One paper accepted to ICLR 2025","type":"talk"},{"authors":["Yifan Wu","Zhiyang Dou","Yuko Ishiwaka","Shun Ogawa","Yuke Lou","Wenping Wang","Lingjie Liu","Taku Komura"],"categories":[],"content":"","date":1733184000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733261690,"objectID":"d9486122b35ec4d0980f82709534b515","permalink":"/publication/cbil-2024/","publishdate":"2024-10-03T21:34:50.388741Z","relpermalink":"/publication/cbil-2024/","section":"publication","summary":"Reproducing realistic collective behaviors presents a captivating yet formidable challenge. Traditional rule-based methods rely on hand-crafted principles, limiting motion diversity and realism in generated collective behaviors. Recent imitation learning methods learn from data but often require ground truth motion trajectories and struggle with authenticity, especially in high-density groups with erratic movements. In this paper, we present a scalable approach, Collective Behavior Imitation Learning (CBIL), for learning fish schooling behavior directly from videos, without relying on captured motion trajectories. Our method first leverages Video Representation Learning, where a Masked Video AutoEncoder (MVAE) extracts implicit states from video inputs in a self-supervised manner. The MVAE effectively maps 2D observations to implicit states that are compact and expressive for following the imitation learning stage. Then, we propose a novel adversarial imitation learning method to effectively capture complex movements of the schools of fish, allowing for efficient imitation of the distribution for motion patterns measured in the latent space. It also incorporates bio-inspired rewards alongside priors to regularize and stabilize training. Once trained, CBIL can be used for various animation tasks with the learned collective motion priors. We further show its effectiveness across different species. Finally, we demonstrate the application of our system in detecting abnormal fish behavior from in-the-wild videos.","tags":["Crowd Animation","Physics-based Character Animation","GAIL","RL"],"title":"CBIL: Collective Behavior Imitation Learning for Fish from Real Videos","type":"publication"},{"authors":["Sinan Wang","Yitong Deng","Molin Deng","Hong-Xing Yu","Junwei Zhou","Duowen Chen","Taku Komura","Jiajun Wu","Bo Zhu"],"categories":[],"content":"","date":1733011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723066490,"objectID":"22c59e8de34dc2b700ec9fd31c1615cd","permalink":"/publication/eulerian-2024/","publishdate":"2021-01-15T21:34:50.388741Z","relpermalink":"/publication/eulerian-2024/","section":"publication","summary":"We present an Eulerian vortex method based on the theory of flow maps to simulate the complex vortical motions of incompressible fluids. Central to our method is the novel incorporation of the flow-map transport equations for line elements, which, in combination with a bi-directional marching scheme for flow maps, enables the high-fidelity Eulerian advection of vorticity variables. The fundamental motivation is that, compared to impulse ùíé, which has been recently bridged with flow maps to encouraging results, vorticity ùùé promises to be preferable for its numerically stability and physical interpretability. To realize the full potential of this novel formulation, we develop a new Poisson solving scheme for vorticity-to-velocity reconstruction that is both efficient and able to accurately handle the coupling near solid boundaries. We demonstrate the efficacy of our approach with a range of vortex simulation examples, including leapfrog vortices, vortex collisions, cavity flow, and the formation of complex vortical structures due to solid-fluid interactions.","tags":["Fluid simulation","Vortex method","Flow map","Grid-based method"],"title":"An Eulerian Vortex Method on Flow Maps","type":"publication"},{"authors":null,"categories":[],"content":"Two papers got accepted at SIGGRAPH Asia 2024. Congratulations!\n","date":1727913600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610746490,"objectID":"de454d5b525d664f2acdea40bd4b340b","permalink":"/talk/202310_sa24/","publishdate":"2024-10-03T00:00:00Z","relpermalink":"/talk/202310_sa24/","section":"talk","summary":"Two papers got accepted to SIGGRAPH Asia 2024. Congratulations!","tags":null,"title":"Two papers accepted to SIGGRAPH Asia 2024","type":"talk"},{"authors":["Huancheng Lin","Floyd M. Chitalu","Taku Komura"],"categories":[],"content":"","date":1723161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610746490,"objectID":"98826162f81ab0ed7d181c721e7324e0","permalink":"/publication/aarap-2024/","publishdate":"2024-08-09T21:34:50.388741Z","relpermalink":"/publication/aarap-2024/","section":"publication","summary":"Anisotropic hyperelastic distortion energies are used to solve many problems in fields like computer graphics and engineering with applications in shape analysis, deformation, design, mesh parameterization, biomechanics and more. However, formulating a robust anisotropic energy that is low-order and yet sufficiently non-linear remains a challenging problem for achieving the convergence promised by Newton-type methods in numerical optimization. In this paper, we propose a novel analytic formulation of an anisotropic energy that is smooth everywhere, low-order, rotationally-invariant and at-least twice differentiable. At its core, our approach utilizes implicit rotation factorizations with invariants of the Cauchy-Green tensor that arises from the deformation gradient. The versatility and generality of our analysis is demonstrated through a variety of examples, where we also show that the constitutive law suggested by the anisotropic version of the well-known \\textit{As-Rigid-As-Possible} energy is the foundational parametric description of both passive and active elastic materials. The generality of our approach means that we can systematically derive the force and force-Jacobian expressions for use in implicit and quasistatic numerical optimization schemes, and we can also use our analysis to rewrite, simplify and speedup several existing anisotropic \\textit{and} isotropic distortion energies with guaranteed inversion-safety.","tags":["Finite Elements","Anisotropy","Orthotropy","Physical Simulation"],"title":"Analytic rotation-invariant modelling of anisotropic finite elements","type":"publication"},{"authors":["Wenyang Zhou","Zhiyang Dou","Zeyu Cao","Zhouyingcheng Liao","Jingbo Wang","Wenjia Wang","Yuan Liu","Taku Komura","Wenping Wang","Lingjie Liu"],"categories":[],"content":"","date":1721606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721684090,"objectID":"93e092370f0f8d29cf4625e395793418","permalink":"/publication/emdm-2024/","publishdate":"2024-07-22T21:34:50.388741Z","relpermalink":"/publication/emdm-2024/","section":"publication","summary":"We introduce Efficient Motion Diffusion Model (EMDM) for fast and high-quality human motion generation. Current state-of-the-art generative diffusion models have produced impressive results but struggle to achieve fast generation without sacrificing quality. On the one hand, previous works, like motion latent diffusion, conduct diffusion within a latent space for efficiency, but learning such a latent space can be a non-trivial effort. On the other hand, accelerating generation by naively increasing the sampling step size, e.g., DDIM, often leads to quality degradation as it fails to approximate the complex denoising distribution. To address these issues, we propose EMDM, which captures the complex distribution during multiple sampling steps in the diffusion model, allowing for much fewer sampling steps and significant acceleration in generation. This is achieved by a conditional denoising diffusion GAN to capture multimodal data distributions among arbitrary (and potentially larger) step sizes conditioned on control signals, enabling fewer-step motion sampling with high fidelity and diversity. To minimize undesired motion artifacts, geometric losses are imposed during network learning. As a result, EMDM achieves real-time motion generation and significantly improves the efficiency of motion diffusion models compared to existing methods while achieving high-quality motion generation. Our code is available at \\url{https://github.com/Frank-ZY-Dou/EMDM}.","tags":["3D human motion generation","Diffusion Model"],"title":"EMDM: Efficient Motion Diffusion Model for Fast and High-Quality Motion Generation","type":"publication"},{"authors":["Zhengming Yu","Zhiyang Dou","Xiaoxiao Long","Cheng Lin","Zekun Li","Yuan Liu","Norman M√ºller","Taku Komura","Marc Habermann","Christian Theobalt","Xin Li","Wenping Wang"],"categories":[],"content":"","date":1721520000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721684090,"objectID":"3b47af34705d2b0b9db47cd4d386bbd9","permalink":"/publication/surfd-2024/","publishdate":"2024-07-22T21:34:50.388741Z","relpermalink":"/publication/surfd-2024/","section":"publication","summary":"We present Surf-D, a novel method for generating high-quality 3D shapes as Surfaces with arbitrary topologies using Diffusion models. Previous methods explored shape generation with different representations and they suffer from limited topologies and poor geometry details. To generate high-quality surfaces of arbitrary topologies, we use the Unsigned Distance Field (UDF) as our surface representation to accommodate arbitrary topologies. Furthermore, we propose a new pipeline that employs a point-based AutoEncoder to learn a compact and continuous latent space for accurately encoding UDF and support high-resolution mesh extraction. We further show that our new pipeline significantly outperforms the prior approaches to learning the distance fields, such as the grid-based AutoEncoder, which is not scalable and incapable of learning accurate UDF. In addition, we adopt a curriculum learning strategy to efficiently embed various surfaces. With the pretrained shape latent space, we employ a latent diffusion model to acquire the distribution of various shapes. Extensive experiments are presented on using Surf-D for unconditional generation, category conditional generation, image conditional generation, and text-to-shape tasks. The experiments demonstrate the superior performance of Surf-D in shape generation across multiple modalities as conditions.","tags":["Shape Generation","Shape Reconstruction","Topology"],"title":"Surf-D: Generating High-Quality Surfaces of Arbitrary Topologies Using Diffusion Models","type":"publication"},{"authors":["Weilin Wan","Zhiyang Dou","Taku Komura","Wenping Wang","Dinesh Jayaraman","Lingjie Liu"],"categories":[],"content":"","date":1721520000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721597690,"objectID":"c3da876c9a8d8ba79d434440590271dd","permalink":"/publication/tlcontrol-2024/","publishdate":"2024-07-21T21:34:50.388741Z","relpermalink":"/publication/tlcontrol-2024/","section":"publication","summary":"Controllable human motion synthesis is essential for applications in AR/VR, gaming, movies, and embodied AI. Existing methods often focus solely on either language or full trajectory control, lacking precision in synthesizing motions aligned with user-specified trajectories, especially for multi-joint control. To address these issues, we present TLControl, a new method for realistic human motion synthesis, incorporating both low-level trajectory and high-level language semantics controls. Specifically, we first train a VQ-VAE to learn a compact latent motion space organized by body parts. We then propose a Masked Trajectories Transformer to make coarse initial predictions of full trajectories of joints based on the learned latent motion space, with user-specified partial trajectories and text descriptions as conditioning. Finally, we introduce an efficient test-time optimization to refine these coarse predictions for accurate trajectory control. Experiments demonstrate that TLControl outperforms the state-of-the-art in trajectory accuracy and time efficiency, making it practical for interactive and high-quality animation generation.","tags":["3D human motion generation"],"title":"TLControl: Trajectory and Language Control for Human Motion Synthesis","type":"publication"},{"authors":null,"categories":[],"content":"SENC, EMDM, Surf-D and TLControl got accepted at ECCV 2024. Congratulations!\n","date":1720569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610746490,"objectID":"056101574455fff5bc054f06f0d5ed1a","permalink":"/talk/202407_eccv/","publishdate":"2024-07-10T00:00:00Z","relpermalink":"/talk/202407_eccv/","section":"talk","summary":"SENC, EMDM, Surf-D and TLControl got accepted at ECCV 2024. Congratulations!","tags":null,"title":"Four papers accepted to ECCV 2024","type":"talk"},{"authors":["Zhouyingcheng Liao","Sinan Wang","Taku Komura"],"categories":[],"content":"","date":1719792000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723066490,"objectID":"5d4c1398fc2b003ba99be114d40db2a5","permalink":"/publication/senc-2024/","publishdate":"2021-01-15T21:34:50.388741Z","relpermalink":"/publication/senc-2024/","section":"publication","summary":"We present SENC, a novel self-supervised neural cloth simulator that addresses the challenge of cloth self-collision. This problem has remained unresolved due to the gap in simulation setup between recent collision detection and response approaches and self-supervised neural simulators. The former requires collision-free initial setups, while the latter necessitates random cloth instantiation during training. To tackle this issue, we propose a novel loss based on Global Intersection Analysis (GIA). This loss extracts the volume surrounded by the cloth region that forms the penetration. By constructing an energy based on this volume, our self-supervised neural simulator can effectively address cloth self-collisions. Moreover, we develop a self-collision-aware graph neural network capable of learning to handle self-collisions, even for parts that are topologically distant from one another. Additionally, we introduce an effective external force scheme that enables the simulation to learn the cloth's behavior in response to random external forces. We validate the efficacy of SENC through extensive quantitative and qualitative experiments, demonstrating that it effectively reduces cloth self-collision while maintaining high-quality animation results.","tags":["Neural cloth simulation","Self collision"],"title":"SENC: Handling Self-collision in Neural Cloth Simulation","type":"publication"},{"authors":["Rui Chen","Mingyi Shi","Shaoli Huang","Ping Tan","Taku Komura","Xuelin Chen"],"categories":[],"content":"","date":1719792000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610746490,"objectID":"792a832d62dcc56303e6cdae04d44c32","permalink":"/publication/camdm-2024/","publishdate":"2024-07-01T21:34:50.388741Z","relpermalink":"/publication/camdm-2024/","section":"publication","summary":"We present a novel character control framework that effectively utilizes motion diffusion probabilistic models to generate high-quality and diverse character animations, responding in real-time to a variety of dynamic user-supplied control signals. At the heart of our method lies a transformer-based Conditional Autoregressive Motion Diffusion Model (CAMDM), which takes as input the character's historical motion and can generate a range of diverse potential future motions conditioned on high-level, coarse user control. To meet the demands for diversity, controllability, and computational efficiency required by a real-time controller, we incorporate several key algorithmic designs. These include separate condition tokenization, classifier-free guidance on past motion, and heuristic future trajectory extension, all designed to address the challenges associated with taming motion diffusion probabilistic models for character control. As a result, our work represents the first model that enables real-time generation of high-quality, diverse character animations based on user interactive control, supporting animating the character in multiple styles with a single unified model. We evaluate our method on a diverse set of locomotion skills, demonstrating the merits of our method over existing character controllers.","tags":["3D human generation","character controller","diffusion model"],"title":"Taming Diffusion Probabilistic Models for Character Control","type":"publication"},{"authors":["Zimeng Wang","Zhiyang Dou","Rui Xu","Cheng Lin","Yuan Liu","Xiaoxiao Long","Shiqing Xin","Taku Komura","Xiaoming Yuan","Wenping Wang"],"categories":[],"content":"","date":1719446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719524090,"objectID":"b249f9670513ab3b9643bbf8b939700c","permalink":"/publication/cat++-2024/","publishdate":"2024-06-27T21:34:50.388741Z","relpermalink":"/publication/cat++-2024/","section":"publication","summary":"We introduce Coverage Axis++, a novel and efficient approach to 3D shape skeletonization. The current state-of-the-art approaches for this task often rely on the watertightness of the input or suffer from substantial computational costs, thereby limiting their practicality. To address this challenge, Coverage Axis++ proposes a heuristic algorithm to select skeletal points, offering a high-accuracy approximation of the Medial Axis Transform (MAT) while significantly mitigating computational intensity for various shape representations. We introduce a simple yet effective strategy that considers shape coverage, uniformity, and centrality to derive skeletal points. The selection procedure enforces consistency with the shape structure while favoring the dominant medial balls, which thus introduces a compact underlying shape representation in terms of MAT. As a result, Coverage Axis++ allows for skeletonization for various shape representations (e.g., water-tight meshes, triangle soups, point clouds), specification of the number of skeletal points, few hyperparameters, and highly efficient computation with improved reconstruction accuracy. Extensive experiments across a wide range of 3D shapes validate the efficiency and effectiveness of Coverage Axis++. \\ZY{Our codes are available at \\url{https://github.com/Frank-ZY-Dou/Coverage_Axis}.","tags":["Geometric Modeling","Medial Axis Transform"],"title":"Coverage Axis++: Efficient Skeletal Points Selection for 3D Shape Skeletonization","type":"publication"},{"authors":null,"categories":[],"content":"One paper accepted at SGP 2024.\n","date":1717200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610746490,"objectID":"7d5f5c08ced7fcfa63d667c54f4b01fb","permalink":"/talk/202406_sgp/","publishdate":"2024-06-01T00:00:00Z","relpermalink":"/talk/202406_sgp/","section":"talk","summary":"One paper accepted to SGP 2024","tags":null,"title":"One paper accepted to SGP 2024","type":"talk"},{"authors":["Kemeng Huang","Floyd M. Chitalu","Huancheng Lin","Taku Komura"],"categories":[],"content":"","date":1711238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610746490,"objectID":"f3b44f7048c3d055f34337f1147007f5","permalink":"/publication/gipc-2024/","publishdate":"2024-03-24T21:34:50.388741Z","relpermalink":"/publication/gipc-2024/","section":"publication","summary":"Barrier functions are crucial for maintaining an intersection- and inversion-free simulation trajectory but existing methods, which directly use distance can restrict implementation design and performance. We present an approach to rewriting the barrier function for arriving at an efficient and robust approximation of its Hessian. The key idea is to formulate a simplicial geometric measure of contact using mesh boundary elements, from which analytic eigensystems are derived and enhanced with filtering and stiffening terms that ensure robustness with respect to the convergence of a Project-Newton solver. A further advantage of our rewriting of the barrier function is that it naturally caters to the notorious case of nearly parallel edge-edge contacts for which we also present a novel analytic eigensystem. Our approach is thus well suited for standard second-order unconstrained optimization strategies for resolving contacts, minimizing nonlinear nonconvex functions where the Hessian may be indefinite. The efficiency of our eigensystems alone yields a 3√ó speedup over the standard Incremental Potential Contact (IPC) barrier formulation. We further apply our analytic proxy eigensystems to produce an entirely GPU-based implementation of IPC with significant further acceleration.","tags":["IPC","Barrier Hessian","Eigen Analysis","GPU"],"title":"GIPC: Fast and Stable Gauss-Newton Optimization of IPC Barrier Energy","type":"publication"},{"authors":["Zhiyang Dou","Xuelin Chen","Qingnan Fan","Taku Komura","Wenping Wang"],"categories":[],"content":"","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610746490,"objectID":"fc9c90001f9e5a64914c8966632228c6","permalink":"/publication/case-2023/","publishdate":"2021-01-15T21:34:50.388741Z","relpermalink":"/publication/case-2023/","section":"publication","summary":"We present C¬∑ASE, an efficient and effective framework that learns Conditional Adversarial Skill Embeddings for physics-based characters. C¬∑ASE enables the physically simulated character to learn a diverse repertoire of skills while providing controllability in the form of direct manipulation of the skills to be performed. This is achieved by dividing the heterogeneous skill motions into distinct subsets containing homogeneous samples for training a low-level conditional model to learn the conditional behavior distribution. The skill-conditioned imitation learning naturally offers explicit control over the character‚Äôs skills after training. The training course incorporates the focal skill sampling, skeletal residual forces, and element-wise feature masking to balance diverse skills of varying complexities, mitigate dynamics mismatch to master agile motions and capture more general behavior characteristics, respectively. Once trained, the conditional model can produce highly diverse and realistic skills, outperforming state-of-the-art models, and can be repurposed in various downstream tasks. In particular, the explicit skill control handle allows a high-level policy or a user to direct the character with desired skill specifications, which we demonstrate is advantageous for interactive character animation.","tags":["Physics-based Character Animation","GAIL","RL"],"title":"C¬∑ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters","type":"publication"},{"authors":["Kunkun Pang","Dafei Qin","Yingruo Fan","Julian Habekost","Takaaki Shiratori","Junichi Yamagishi","Taku Komura"],"categories":[],"content":"","date":1691366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610746490,"objectID":"55844c27c5a5f6aba4c2f70692d72bd8","permalink":"/publication/bodyformer/","publishdate":"2021-01-15T21:34:50.388741Z","relpermalink":"/publication/bodyformer/","section":"publication","summary":"Automatic gesture synthesis from speech is a topic that has attracted researchers for applications in remote communication, video games and Metaverse. Learning the mapping between speech and 3D full-body gestures is difficult due to the stochastic nature of the problem and the lack of a rich cross-modal dataset that is needed for training. In this paper, we propose a novel transformer-based framework for automatic 3D body gesture synthesis from speech. To learn the stochastic nature of the body gesture during speech, we propose a variational transformer to effectively model a probabilistic distribution over gestures, which can produce diverse gestures during inference. Furthermore, we introduce a mode positional embedding layer to capture the different motion speeds in different speaking modes. To cope with the scarcity of data, we design an intra-modal pre-training scheme that can learn the complex mapping between the speech and the 3D gesture from a limited amount of data. Our system is trained with either the Trinity speech-gesture dataset or the Talking With Hands 16.2M dataset. The results show that our system can produce more realistic, appropriate, and diverse body gestures compared to existing state-of-the-art approaches.","tags":["Motion Generation","Computer Animation"],"title":"Bodyformer: Semantics-guided 3D Body Gesture Synthesis With Transformer","type":"publication"},{"authors":["Dafei Qin","Jun Saito","Noam Aigerman","Thibault Groueix","Taku Komura"],"categories":[],"content":"","date":1691366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610746490,"objectID":"f20bfbdc8ce33f4930d4af7e586fa3ae","permalink":"/publication/nfr/","publishdate":"2021-01-15T21:34:50.388741Z","relpermalink":"/publication/nfr/","section":"publication","summary":"We propose an end-to-end deep-learning approach for automatic rigging and retargeting of 3D models of human faces in the wild. Our approach, called Neural Face Rigging \\(NFR\\), holds three key properties\\:\n\\(i\\) NFR's expression space maintains human-interpretable editing parameters for artistic controls;\n\\(ii\\) NFR is readily applicable to arbitrary facial meshes with different connectivity and expressions;\n\\(iii\\) NFR can encode and produce fine-grained details of complex expressions performed by arbitrary subjects.\nTo the best of our knowledge, NFR is the first approach to provide realistic and controllable deformations of in-the-wild facial meshes, without the manual creation of blendshapes or correspondence. We design a deformation autoencoder and train it through a multi-dataset training scheme, which benefits from the unique advantages of two data sources\\:a linear 3DMM with interpretable control parameters as in FACS, and 4D captures of real faces with fine-grained details. Through various experiments, we show NFR's ability to automatically produce realistic and accurate facial deformations across a wide range of existing datasets as well as noisy facial scans in-the-wild, while providing artist-controlled, editable parameters.","tags":["Facial Rigging","Facial Animation","Retargeting"],"title":"Neural Face Rigging for Animating and Retargeting Facial Meshes in the Wild","type":"publication"},{"authors":null,"categories":[],"content":"Four papers were accepted at ICCV 2023. Congratulations!\nZolly: Zoom Focal Length Correctly for Perspective-Distorted Human Mesh Reconstruction\nPhaseMP: Robust 3D Pose Estimation via Phase-conditioned Human Motion Prior \nTORE: Token Reduction for Efficient Human Mesh Recovery with Transformer \nSurface Extraction from Neural Unsigned Distance Fields\n","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610746490,"objectID":"a994e6c5dcebcc408719a8f0c3b16eca","permalink":"/talk/202307_iccv/","publishdate":"2023-07-01T00:00:00Z","relpermalink":"/talk/202307_iccv/","section":"talk","summary":"Zolly, PhaseMP, TORE, DualMeshUDF were accepted at ICCV 2023. Congratulations!","tags":null,"title":"Four papers accepted to ICCV 2023","type":"talk"},{"authors":["Mingyi Shi","Sebastian Starke","Yuting Ye","Taku Komura","Jungdam Won"],"categories":[],"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610746490,"objectID":"1a05468a4a025b13d84c0bf13f86c969","permalink":"/publication/phasemp-2023/","publishdate":"2021-01-15T21:34:50.388741Z","relpermalink":"/publication/phasemp-2023/","section":"publication","summary":"We present a novel motion prior, called PhaseMP , modeling a probability distribution on pose transitions conditioned by a frequency domain feature extracted from a periodic autoencoder. The phase feature further enforces the pose transitions to be unidirectional (i.e. no backward movement in time), from which more stable and natural motions can be generated. Specifically, our motion prior can be useful for accurately estimating 3D human motions in the presence of challenging input data, including long periods of spatial and temporal occlusion, as well as noisy sensor measurements. Through a comprehensive evaluation, we demonstrate the efficacy of our novel motion prior, showcasing its superiority over existing state-of-the-art methods by a significant margin across various applications, including video-to-motion and motion estimation from sparse sensor data, and etc.","tags":["3D human reconstruction"],"title":"PhaseMP: Robust 3D Pose Estimation via Phase-conditioned Human Motion Prior","type":"publication"},{"authors":["Wenjia Wang","Yongtao Ge","Haiyi Mei","Zhongang Cai","Qingping Sun","Yanjun Wang","Chunhua Shen","Lei Yang","Taku Komura"],"categories":[],"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610746490,"objectID":"8508bb435de53de310010c6a492e18e3","permalink":"/publication/zolly-2023/","publishdate":"2021-01-15T21:34:50.388741Z","relpermalink":"/publication/zolly-2023/","section":"publication","summary":"As it is hard to calibrate single-view RGB images in the wild, existing 3D human mesh reconstruction (3DHMR) methods either use a constant large focal length or estimate one based on the background environment context, which can not tackle the problem of the torso, limb, hand or face distortion caused by perspective camera projection when the camera is close to the human body. The naive focal length assumptions can harm this task with the incorrectly formulated projection matrices. To solve this, we propose Zolly, the first 3DHMR method focusing on perspective-distorted images. Our approach begins with analysing the reason for perspective distortion, which we find is mainly caused by the relative location of the human body to the camera center. We propose a new camera model and a novel 2D representation, termed distortion image, which describes the 2D dense distortion scale of the human body. We then estimate the distance from distortion scale features rather than environment context features. Afterwards, We integrate the distortion feature with image features to reconstruct the body mesh. To formulate the correct projection matrix and locate the human body position, we simultaneously use perspective and weak-perspective projection loss. Since existing datasets could not handle this task, we propose the first synthetic dataset PDHuman and extend two real-world datasets tailored for this task, all containing perspective-distorted human images. Extensive experiments show that Zolly outperforms existing state-of-the-art methods on both perspective-distorted datasets and the standard benchmark (3DPW).","tags":["3D human reconstruction","Perspective camera"],"title":"Zolly: Zoom Focal Length Correctly for Perspective-Distorted Human Mesh Reconstruction","type":"publication"},{"authors":["Zhiyang Dou","Qingxuan Wu","Cheng Lin","Zeyu Cao","Qiangqiang Wu","Weilin Wan","Taku Komura","Wenping Wang"],"categories":[],"content":"","date":1685577600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610746490,"objectID":"420ca8442061abf856b867fdd6d1178b","permalink":"/publication/tore-2023/","publishdate":"2021-01-15T21:34:50.388741Z","relpermalink":"/publication/tore-2023/","section":"publication","summary":"In this paper, we introduce a set of simple yet effective TOken REduction (TORE) strategies for Transformer-based Human Mesh Recovery from monocular images. Current SOTA performance is achieved by Transformer-based structures. However, they suffer from high model complexity and computation cost caused by redundant tokens. We propose token reduction strategies based on two important aspects, i.e., the 3D geometry structure and 2D image feature, where we hierarchically recover the mesh geometry with priors from body structure and conduct token clustering to pass fewer but more discriminative image feature tokens to the Transformer. Our method massively reduces the number of tokens involved in high-complexity interactions in the Transformer. This leads to a significantly reduced computational cost while still achieving competitive or even higher accuracy in shape recovery. Extensive experiments across a wide range of benchmarks validate the superior effectiveness of the proposed method. We further demonstrate the generalizability of our method on hand mesh recovery. Our code will be publicly available once the paper is published.","tags":["3D human reconstruction","Token reduction"],"title":"TORE: Token Reduction for Efficient Human Mesh Recovery with Transformer","type":"publication"},{"authors":["Huancheng Lin","Floyd M. Chitalu","Taku Komura"],"categories":[],"content":"","date":1669766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610746490,"objectID":"e75f830493febeb654d8a1b0bdc590f0","permalink":"/publication/iarap-2022/","publishdate":"2022-11-30T21:34:50.388741Z","relpermalink":"/publication/iarap-2022/","section":"publication","summary":"Isotropic As-Rigid-As-Possible (ARAP) energy has been popular for shape editing, mesh parametrisation and soft-body simulation for almost two decades. However, a formulation using Cauchy-Green (CG) invariants has always been unclear, due to a rotation-polluted trace term that cannot be directly expressed using these invariants. We show how this incongruent trace term can be understood via an implicit relationship to the CG invariants. Our analysis reveals this relationship to be a polynomial where the roots equate to the trace term, and where the derivatives also give rise to closed-form expressions of the Hessian to guarantee positive semi-definiteness for a fast and concise Newton-type implicit time integration. A consequence of this analysis is a novel analytical formulation to compute rotations and singular values of deformation-gradient tensors without explicit/numerical factorization, which is significant, resulting in up to 3.5x speedup and benefits energy function evaluation for reducing solver time. We validate our energy formulation by experiments and comparison, demonstrating that our resulting eigendecomposition using the CG invariants is equivalent to existing ARAP formulations. We thus reveal isotropic ARAP energy to be a member of the \"Cauchy-Green club\", meaning that it can indeed be defined using CG invariants and therefore that the closed-form expressions of the resulting Hessian are shared with other energies written in their terms.","tags":["Mesh geometry models","Physical Simulation"],"title":"Isotropic ARAP energy using Cauchy-Green invariants","type":"publication"},{"authors":["Zhiyang Dou","Cheng Lin","Rui Xu","Lei Yang","Shiqing Xin","Taku Komura","Wenping Wang"],"categories":[],"content":"","date":1648771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610746490,"objectID":"17548a70ad29d93408f8636571decd9a","permalink":"/publication/cat-2022/","publishdate":"2021-01-15T21:34:50.388741Z","relpermalink":"/publication/cat-2022/","section":"publication","summary":"In this paper, we present a simple yet effective formulation called Coverage Axis for 3D shape skeletonization. Inspired by the set cover problem, our key idea is to cover all the surface points using as few inside medial balls as possible. This formulation inherently induces a compact and expressive approximation of the Medial Axis Transform (MAT) of a given shape. Different from previous methods that rely on local approximation error, our method allows a global consideration of the overall shape structure, leading to an efficient high-level abstraction and superior robustness to noise. Another appealing aspect of our method is its capability to handle more generalized input such as point clouds and poor-quality meshes. Extensive comparisons and evaluations demonstrate the remarkable effectiveness of our method for generating compact and expressive skeletal representation to approximate the MAT.","tags":["Geometric Modeling","Medial Axis Transform"],"title":"Coverage Axis: Inner Point Selection for 3D Shape Skeletonization","type":"publication"},{"authors":["Jingyuan Liu","Mingyi Shi","Qifeng Chen","Hongbo Fu","Chiew-Lan Tai"],"categories":[],"content":"","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625175290,"objectID":"6610b4f34f90bfab72546c33ec54febd","permalink":"/publication/normpose-2021/","publishdate":"2021-07-01T21:34:50.388741Z","relpermalink":"/publication/normpose-2021/","section":"publication","summary":"We present a novel approach for extracting human pose features from human action videos. The goal is to let the pose features capture only the poses of the action while being invariant to other factors, including video backgrounds, the video subjects‚Äô anthropometric characteristics and viewpoints. Such human pose features facilitate the comparison of pose similarity and can be used for downstream tasks, such as human action video alignment and pose retrieval. The key to our approach is to first normalize the poses in the video frames by mapping the poses onto a pre-defined 3D skeleton to not only disentangle subject physical features, such as bone lengths and ratios, but also to unify global orientations of the poses. Then the normalized poses are mapped to a pose embedding space of highlevel features, learned via unsupervised metric learning. We evaluate the effectiveness of our normalized features both qualitatively by visualizations, and quantitatively by a video alignment task on the Human3.6M dataset and an action recognition task on the Penn Action dataset.","tags":["Human Action Recognization"],"title":"Normalized Human Pose Features for Human Action Video Alignment","type":"publication"},{"authors":["Mingyi Shi","Kfir Aberman","Andreas Aristidou","Taku Komura","Dani Lischinski","Daniel Cohen-Or","Baoquan Chen"],"categories":[],"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579124090,"objectID":"1527eb7a180f5b57e788a48f716d9b58","permalink":"/publication/motionet-2020/","publishdate":"2020-07-01T21:34:50.388741Z","relpermalink":"/publication/motionet-2020/","section":"publication","summary":"We introduce MotioNet, a deep neural network that directly reconstructs the motion of a 3D human skeleton from monocular video. While previous methods rely on either rigging or inverse kinematics (IK) to associate a consistent skeleton with temporally coherent joint rotations, our method is the first data-driven approach that directly outputs a kinematic skeleton, which is a complete, commonly used, motion representation. At the crux of our approach lies a deep neural network with embedded kinematic priors, which decomposes sequences of 2D joint positions into two separate attributes - a single, symmetric, skeleton, encoded by bone lengths, and a sequence of 3D joint rotations associated with global root positions and foot contact labels.","tags":["3D motion reconstruction"],"title":"MotioNet: 3D Human Motion Reconstruction from Monocular Video with Skeleton Consistency","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"300db838c7a68c4fd14037dd038d2e1c","permalink":"/courses/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"60abaeb405147d861bb55801d25d30df","permalink":"/research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7d752e9f9c21080440d4436b30b9351b","permalink":"/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"People","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"850d3285b9d4b0dba29acf3237d91a0c","permalink":"/tools/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/tools/","section":"","summary":"","tags":null,"title":"Tools","type":"widget_page"}]