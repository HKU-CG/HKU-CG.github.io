<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 4.8.0 for Hugo"><meta name=author content="Computer Graphics and Visualization Lab"><meta name=description content><link rel=alternate hreflang=en-us href=/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#2962ff"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin=anonymous async></script><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap"><link rel=stylesheet href=/css/wowchemy.css><script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script><link rel=alternate href=/index.xml type=application/rss+xml title="HKU CGVU Lab"><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/images/icon_hu2e4579b938cb71ab526c5021d9215e0c_79015_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/images/icon_hu2e4579b938cb71ab526c5021d9215e0c_79015_192x192_fill_lanczos_center_2.png><link rel=canonical href=/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="HKU CGVU Lab"><meta property="og:url" content="/"><meta property="og:title" content="HKU CGVU Lab"><meta property="og:description" content><meta property="og:image" content="/images/logo_hu2e4579b938cb71ab526c5021d9215e0c_79015_300x300_fit_lanczos_2.png"><meta property="twitter:image" content="/images/logo_hu2e4579b938cb71ab526c5021d9215e0c_79015_300x300_fit_lanczos_2.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2025-03-01T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"/?q={search_term_string}","query-input":"required name=search_term_string"},"url":"/"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","@id":"/","name":"HKU CGVU Lab","logo":"/images/logo_hu2e4579b938cb71ab526c5021d9215e0c_79015_192x192_fit_lanczos_2.png","address":{"@type":"PostalAddress","streetAddress":"CYC Building, the University of Hong Kong","addressLocality":"Hong Kong","addressRegion":"","postalCode":"","addressCountry":"HK"},"url":"/"}</script><title>HKU CGVU Lab</title></head><body id=top data-spy=scroll data-offset=70 data-target=#navbar-main><script>window.wcDarkLightEnabled=true;</script><script>const isSiteThemeDark=false;</script><script src=/js/load-theme.js></script><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/><img src=/images/logo_hu2e4579b938cb71ab526c5021d9215e0c_79015_0x70_resize_lanczos_2.png alt="HKU CGVU Lab"></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/><img src=/images/logo_hu2e4579b938cb71ab526c5021d9215e0c_79015_0x70_resize_lanczos_2.png alt="HKU CGVU Lab"></a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about data-target=#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/people><span>People</span></a></li><li class=nav-item><a class=nav-link href=/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/courses><span>Teaching</span></a></li><li class=nav-item><a class=nav-link href=/#contact data-target=#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav><span class="js-widget-page d-none"></span><section id=about class="home-section wg-about"><div class=container><div class=row><div class="col-12 col-lg-4"><div id=profile><a href><img class="avatar avatar-circle" src=/author/computer-graphics-and-visualization-lab/avatar_hu2e4579b938cb71ab526c5021d9215e0c_79015_270x270_fill_lanczos_center_2.png alt="Computer Graphics and Visualization Lab"></a><div class=portrait-title><h2>Computer Graphics and Visualization Lab</h2><h3><span>University of Hong Kong</span></h3></div><ul class=network-icon aria-hidden=true><li><a href=/#contact><i class="fas fa-envelope"></i></a></li><li><a href=https://github.com/MyersResearchGroup target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul></div></div><div class="col-12 col-lg-8"><h1>Computer Graphics and Visualization</h1><p>CGVU Lab, led by Prof. <a href=author/taku-komura>Taku Komura</a>, belongs to the Department of Computer Science, the University of Hong Kong. Our research focus is on physically-based animation and the application of machine learning techniques for animation synthesis.</p><p><img src=/media/group_small.jpg alt=group.png></p><div class=row></div></div></div></div></section><section id=talks class="home-section wg-pages"><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>News</h1></div><div class="col-12 col-lg-8"><div class="media stream-item"><div class=media-body><h3 class="article-title mb-0 mt-0"><a href=/talk/202407_siggraph/>One paper accepted to ICLR 2025</a></h3><a href=/talk/202407_siggraph/ class=summary-link><div class=article-style>Check our new paper: DICE.</div></a><div class="stream-meta article-metadata"><div><span>Jan 1, 2025 12:00 AM</span></div></div><div class=btn-links></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><h3 class="article-title mb-0 mt-0"><a href=/talk/202310_sa24/>Two papers accepted to SIGGRAPH Asia 2024</a></h3><a href=/talk/202310_sa24/ class=summary-link><div class=article-style>Two papers got accepted to SIGGRAPH Asia 2024. Congratulations!</div></a><div class="stream-meta article-metadata"><div><span>Oct 3, 2024 12:00 AM</span></div></div><div class=btn-links></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><h3 class="article-title mb-0 mt-0"><a href=/talk/202407_eccv/>Four papers accepted to ECCV 2024</a></h3><a href=/talk/202407_eccv/ class=summary-link><div class=article-style>SENC, EMDM, Surf-D and TLControl got accepted at ECCV 2024. Congratulations!</div></a><div class="stream-meta article-metadata"><div><span>Jul 10, 2024 12:00 AM</span></div></div><div class=btn-links></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><h3 class="article-title mb-0 mt-0"><a href=/talk/202406_sgp/>One paper accepted to SGP 2024</a></h3><a href=/talk/202406_sgp/ class=summary-link><div class=article-style>One paper accepted to SGP 2024</div></a><div class="stream-meta article-metadata"><div><span>Jun 1, 2024 12:00 AM</span></div></div><div class=btn-links></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><h3 class="article-title mb-0 mt-0"><a href=/talk/202307_iccv/>Four papers accepted to ICCV 2023</a></h3><a href=/talk/202307_iccv/ class=summary-link><div class=article-style>Zolly, PhaseMP, TORE, DualMeshUDF were accepted at ICCV 2023. Congratulations!</div></a><div class="stream-meta article-metadata"><div><span>Jul 1, 2023 12:00 AM</span></div></div><div class=btn-links></div></div><div class=ml-3></div></div></div></div></div></section><section id=people class="home-section wg-people"><div class=container><div class="row justify-content-center people-widget"><div class="col-md-12 section-heading"><h1>Meet the Team</h1></div><div class=col-md-12><h2 class=mb-4>Principal Investigator</h2></div><div class="col-12 col-sm-auto people-person"><a href=/author/taku-komura/><img class="avatar avatar-circle" src=/author/taku-komura/avatar_hu13dcf3a41fad6ceb125ba13611db0352_110674_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/taku-komura/>Taku Komura</a></h2><h3>Professor</h3><ul class=network-icon aria-hidden=true><li><a href=mailto:taku@cs.hku.hk><i class="fas fa-envelope"></i></a></li></ul><p class=people-interests>Physical Simulation, Character Animation, 3D Modelling</p></div></div><div class=col-md-12><h2 class=mb-4>Research Staff</h2></div><div class="col-12 col-sm-auto people-person"><a href=/author/floyd-m.-chitalu/><img class="avatar avatar-circle" src=/author/floyd-m.-chitalu/avatar_hu1063771309584274d52adb7cafa1bff9_372560_270x270_fill_q90_lanczos_center.JPG alt=Avatar></a><div class=portrait-title><h2><a href=/author/floyd-m.-chitalu/>Floyd M. Chitalu</a></h2><h3>Senior researcher, since Nov. 2022.</h3><ul class=network-icon aria-hidden=true><li><a href=mailto:floyd.m.chitalu@gmail.com><i class="fas fa-envelope"></i></a></li><li><a href=https://orcid.org/0000-0001-9489-8592 target=_blank rel=noopener><i class="fab fa-orcid"></i></a></li></ul><p class=people-interests>Physical Simulation</p></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/yinghao-huang/><img class="avatar avatar-circle" src=/author/yinghao-huang/avatar_hu81e4ac04fbbc03f89662d1d2750f0bdc_5094_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/yinghao-huang/>Yinghao Huang</a></h2><h3>Postdoc, since Aug. 2023.</h3><ul class=network-icon aria-hidden=true><li><a href=https://github.com/MuGdxy target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul><p class=people-interests>Human Pose Estimation, Human Motion Generation</p></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/chen-peng/><img class="avatar avatar-circle" src=/author/chen-peng/avatar_hu49e4e550a0562da8b08f9ce3be54c0ae_9533_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/chen-peng/>Chen Peng</a></h2><h3>Postdoc, since Sep. 2023.</h3><ul class=network-icon aria-hidden=true><li><a href=https://github.com/MuGdxy target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul><p class=people-interests>Physically-Based Animation, Fluid Simulation</p></div></div><div class=col-md-12><h2 class=mb-4>Graduate Students</h2></div><div class="col-12 col-sm-auto people-person"><a href=/author/linxu-fan/><img class="avatar avatar-circle" src=/author/linxu-fan/avatar_hu50664642510d1e90cbd1c6a26672845a_7525389_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/linxu-fan/>Linxu Fan</a></h2><h3>PHD, since Nov. 2019.</h3><ul class=network-icon aria-hidden=true><li><a href=https://github.com/Linxu-Fan target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul><p class=people-interests>Physical Simulation</p></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/zhiyang-dou/><img class="avatar avatar-circle" src=/author/zhiyang-dou/avatar_huabca85c93043588a4748def6dc5b0774_41772_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/zhiyang-dou/>Zhiyang Dou</a></h2><h3>PhD, since Aug. 2020.<br>Co-supv. by Prof. Wenping Wang.</h3><ul class=network-icon aria-hidden=true><li><a href="https://scholar.google.com/citations?user=SLRYlKsAAAAJ&hl=en" target=_blank rel=noopener><i class="fas fa-user-graduate"></i></a></li><li><a href=https://github.com/Frank-ZY-Dou target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.colorado.edu/cs/academics/ target=_blank rel=noopener><i class="Custom_University Custom_University-university-logo"></i></a></li></ul><p class=people-interests>Character Animation, Geometric Computing</p></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/dafei-qin/><img class="avatar avatar-circle" src=/author/dafei-qin/avatar_huc7eaf55a7a9d267e4b0828a780ebfe27_214218_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/dafei-qin/>Dafei Qin</a></h2><h3>PhD, since Sep. 2020.</h3><ul class=network-icon aria-hidden=true><li><a href=mailto:qindfei@connect.hku.hk><i class="fas fa-envelope"></i></a></li><li><a href=/dafei-qin.github.io><i class="fas fa-user-graduate"></i></a></li><li><a href=https://github.com/dafei-qin target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul><p class=people-interests>Facial Animation, Neural Rendering</p></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/mingyi-shi/><img class="avatar avatar-circle" src=/author/mingyi-shi/avatar_hufc259471fefad5e040bf12ae1eefbb7a_250718_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/mingyi-shi/>Mingyi Shi</a></h2><h3>PhD, since Nov. 2020.</h3><ul class=network-icon aria-hidden=true><li><a href="https://scholar.google.com/citations?user=50MVe54AAAAJ&hl=en&oi=ao" target=_blank rel=noopener><i class="fas fa-user-graduate"></i></a></li><li><a href=https://github.com/Shimingyi target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://orcid.org/0000-0002-5180-600X target=_blank rel=noopener><i class="fab fa-orcid"></i></a></li><li><a href=mailto:myshi@cs.hku.hk><i class="fas fa-envelope"></i></a></li></ul><p class=people-interests>3D Human Moton, Generative AI</p></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/jintao-lu/><img class="avatar avatar-circle" src=/author/jintao-lu/avatar_hu65e09ee240fa88c8efa044040b40db39_225040_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/jintao-lu/>Jintao Lu</a></h2><h3>PhD, since Sept. 2021.</h3><ul class=network-icon aria-hidden=true><li><a href="https://scholar.google.com/citations?user=0AwDFzkAAAAJ&hl=en&oi=ao" target=_blank rel=noopener><i class="fas fa-user-graduate"></i></a></li><li><a href=https://github.com/lujintaozju target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.colorado.edu/cs/academics/ target=_blank rel=noopener><i class="Custom_University Custom_University-university-logo"></i></a></li></ul><p class=people-interests>Human Scene Interaction, Motion Control</p></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/huancheng-lin/><img class="avatar avatar-circle" src=/author/huancheng-lin/avatar_hu10f78f12553060e22505c0d745b869eb_887352_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/huancheng-lin/>Huancheng Lin</a></h2><h3>M.Phil., since Sep. 2022.</h3><ul class=network-icon aria-hidden=true><li><a href=https://github.com/LamWS target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul><p class=people-interests>Physical Simulation</p></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/kemeng-huang/><img class="avatar avatar-circle" src=/author/kemeng-huang/avatar_hud601aad252c4ce4ddc0d3635ede9ad85_236903_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/kemeng-huang/>Kemeng Huang</a></h2><h3>PhD, since Sep. 2022.</h3><ul class=network-icon aria-hidden=true><li><a href="https://scholar.google.com/citations?user=g9JBjacAAAAJ&hl=en" target=_blank rel=noopener><i class="fas fa-user-graduate"></i></a></li><li><a href=https://github.com/KemengHuang target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://orcid.org/0000-0001-9147-2289 target=_blank rel=noopener><i class="fab fa-orcid"></i></a></li><li><a href=/kmhuang@connect.hku.hk><i class="fas fa-envelope"></i></a></li></ul><p class=people-interests>Physical Simulation, High Performance Computing</p></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/guying-lin/><img class="avatar avatar-circle" src=/author/guying-lin/avatar_hu4e769929195e686a948b54985ea7bfc2_1179400_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/guying-lin/>Guying Lin</a></h2><h3>MPhil, since Sept. 2022.<br>Co-supv. by Prof. Wenping Wang.</h3><ul class=network-icon aria-hidden=true><li><a href="https://scholar.google.com/citations?hl=zh-CN&view_op=list_works&gmla=AOV7GLPL5_aNmVXVpckO8UhukOwC6uTHFcZbwLiuA1BnBUBpKin05xlYFzaUBt1GBk6bYOFN_ViTKQUmsn803qtbj1A&user=B4XcF1AAAAAJ" target=_blank rel=noopener><i class="fas fa-user-graduate"></i></a></li><li><a href=https://github.com/Carrie-lin target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul><p class=people-interests>Neural Implicit Surface Representation</p></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/wenjia-wang/><img class="avatar avatar-circle" src=/author/wenjia-wang/avatar_hu3a35180cd8005dbbaa8c7398fcceaa0c_3237495_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/wenjia-wang/>Wenjia Wang</a></h2><h3>PhD, since Jan. 2023.</h3><ul class=network-icon aria-hidden=true><li><a href=https://wenjiawang0312.github.io/ target=_blank rel=noopener><i class="fas fa-user-graduate"></i></a></li><li><a href=https://github.com/WenjiaWang0312 target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=mailto:wwj2022@connect.hku.hk><i class="fas fa-envelope"></i></a></li></ul><p class=people-interests>3D Reconstruction, Human Pose Estimation, Human Motion Generation</p></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/zhouyingcheng-liao/><img class="avatar avatar-circle" src=/author/zhouyingcheng-liao/avatar_hu4386e5d2358330d0283e784e353c273d_56561_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/zhouyingcheng-liao/>Zhouyingcheng Liao</a></h2><h3>PhD, since Jan. 2023.</h3><ul class=network-icon aria-hidden=true><li><a href=mailto:zycliao@cs.hku.hk><i class="fas fa-envelope"></i></a></li><li><a href=https://zycliao.com target=_blank rel=noopener><i class="fas fa-user-graduate"></i></a></li><li><a href=https://github.com/zycliao target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul><p class=people-interests>Neural Cloth Simulation, Character Animation</p></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/yuke-lou/><img class="avatar avatar-circle" src=/author/yuke-lou/avatar_hua1a0ddd381cf10c76e8f2d5e7c72684b_1866275_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/yuke-lou/>Yuke Lou</a></h2><h3>M.Phil, since Sept. 2023.</h3><ul class=network-icon aria-hidden=true><li><a href=https://thorin666.github.io/ target=_blank rel=noopener><i class="fas fa-user-graduate"></i></a></li><li><a href=https://github.com/thorin666 target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.colorado.edu/cs/academics/ target=_blank rel=noopener><i class="Custom_University Custom_University-university-logo"></i></a></li></ul><p class=people-interests>Motion Generation</p></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/xiaohan-ye/><img class="avatar avatar-circle" src=/author/xiaohan-ye/avatar_hu08a423740bce08d830e5f1631fe29adf_1452215_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/xiaohan-ye/>Xiaohan Ye</a></h2><h3>PhD, since Sept. 2023.</h3><ul class=network-icon aria-hidden=true><li><a href=https://github.com/XiaohanYE99 target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.colorado.edu/cs/academics/ target=_blank rel=noopener><i class="Custom_University Custom_University-university-logo"></i></a></li></ul><p class=people-interests>Physics Simulation, Motion Control</p></div></div><div class=col-md-12><h2 class=mb-4>Research Assistant</h2></div><div class="col-12 col-sm-auto people-person"><a href=/author/leo-ho/><img class="avatar avatar-circle" src=/author/leo-ho/avatar_hue24e034583338e4e9706cfe6ef05ef94_809271_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/leo-ho/>Leo Ho</a></h2><h3>Research Assistant, since Aug. 2023.</h3><ul class=network-icon aria-hidden=true><li><a href=mailto:leohocs@hku.hk><i class="fas fa-envelope"></i></a></li><li><a href=https://github.com/leohku target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul><p class=people-interests>Digital Humans, Motion Synthesis</p></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/xinyu-lu/><img class="avatar avatar-circle" src=/author/xinyu-lu/avatar_hu99b47cd33cd44b1e503fb3f1c96378e9_1115449_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/xinyu-lu/>Xinyu Lu</a></h2><h3>Research Assistant, since Sep. 2023.</h3><ul class=network-icon aria-hidden=true><li><a href=https://github.com/MuGdxy target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul><p class=people-interests>Physically-Based Animation, Simulation</p></div></div></div></div></section><section id=publications class="home-section wg-pages"><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Recent Publications</h1></div><div class="col-12 col-lg-8"><div class="alert alert-note"><div>Quickly discover relevant content by <a href=/publication/>filtering publications</a>.</div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/liang-pan/>Liang Pan</a></span>, <span><a href=/author/zeshi-yang/>Zeshi Yang</a></span>, <span><a href=/author/zhiyang-dou/>Zhiyang Dou</a></span>, <span><a href=/author/wenjia-wang/>Wenjia Wang</a></span>, <span><a href=/author/buzhen-huang/>Buzhen Huang</a></span>, <span><a href=/author/bo-dai/>Bo Dai</a></span>, <span><a href=/author/taku-komura/>Taku Komura</a></span>, <span><a href=/author/jingbo-wang/>Jingbo Wang</a></span></div><span class=article-date>March 2025</span>
<span class=middot-divider></span><span class=pub-publication>CVPR 2025</span></div><a href=/publication/tokenhsi-2025/><img src=/publication/tokenhsi-2025/featured_hu1f92fe34406de72f44899aa407a19013_3089942_600x400_fit_lanczos_2.png class=article-banner alt="TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization"></a><h3 class="article-title mb-1 mt-3"><a href=/publication/tokenhsi-2025/>TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization</a></h3><a href=/publication/tokenhsi-2025/ class=summary-link><div class=article-style><p>The synthesis of realistic and physically plausible human-scene interaction animations presents a critical and complex challenge in computer vision and embodied AI. Recent advances primarily focus on developing specialized character controllers for individual interaction tasks, such as contacting and carrying, often overlooking the need to establish a unified policy for versatile skills. This limitation hinders the ability to generate high-quality motions across a variety of challenging human-scene interaction tasks that require the integration of multiple skills, e.g., walking to a chair and sitting down while carrying a box. To address this issue, we present TokenHSI, a unified controller designed to synthesize various types of human-scene interaction animations. The key innovation of our framework is the use of tokenized proprioception for the simulated character, combined with various task observations, complemented by a masking mechanism that enables the selection of tasks on demand. In addition, our unified policy network is equipped with flexible input size capabilities, enabling efficient adaptation of learned foundational skills to new environments and tasks. By introducing additional input tokens to the pre-trained policy, we can not only modify interaction targets but also integrate learned skills to address diverse challenges. Overall, our framework facilitates the generation of a wide range of character animations, significantly improving flexibility and adaptability in human-scene interactions.</p></div></a><div class=btn-links></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/qingxuan-wu/>Qingxuan Wu</a></span>, <span><a href=/author/zhiyang-dou/>Zhiyang Dou</a></span>, <span><a href=/author/sirui-xu/>Sirui Xu</a></span>, <span><a href=/author/soshi-shimada/>Soshi Shimada</a></span>, <span><a href=/author/chen-wang/>Chen Wang</a></span>, <span><a href=/author/zhengming-yu/>Zhengming Yu</a></span>, <span><a href=/author/yuan-liu/>Yuan Liu</a></span>, <span><a href=/author/cheng-lin/>Cheng Lin</a></span>, <span><a href=/author/zeyu-cao/>Zeyu Cao</a></span>, <span><a href=/author/taku-komura/>Taku Komura</a></span>, <span><a href=/author/vladislav-golyanik/>Vladislav Golyanik</a></span>, <span><a href=/author/christian-theobalt/>Christian Theobalt</a></span>, <span><a href=/author/wenping-wang/>Wenping Wang</a></span>, <span><a href=/author/lingjie-liu/>Lingjie Liu</a></span></div><span class=article-date>January 2025</span>
<span class=middot-divider></span><span class=pub-publication>ICLR 2025</span></div><a href=/publication/dice-2025/><img src=/publication/dice-2025/featured_hu5114f6f136f898fd74f0eacf05cf8e1d_1512188_600x400_fit_lanczos_2.png class=article-banner alt="DICE: End-to-end Deformation Capture of Hand-Face Interactions from a Single Image"></a><h3 class="article-title mb-1 mt-3"><a href=/publication/dice-2025/>DICE: End-to-end Deformation Capture of Hand-Face Interactions from a Single Image</a></h3><a href=/publication/dice-2025/ class=summary-link><div class=article-style><p>Reconstructing 3D hand-face interactions with deformations from a single image is a challenging yet crucial task with broad applications in AR, VR, and gaming. The challenges stem from self-occlusions during single-view hand-face interactions, diverse spatial relationships between hands and face, complex deformations, and the ambiguity of the single-view setting. The first and only method for hand-face interaction recovery, Decaf, introduces a global fitting optimization guided by contact and deformation estimation networks trained on studio-collected data with 3D annotations. However, Decaf suffers from a time-consuming optimization process and limited generalization capability due to its reliance on 3D annotations of hand-face interaction data. To address these issues, we present DICE, the first end-to-end method for Deformation-aware hand-face Interaction reCovEry from a single image. DICE estimates the poses of hands and faces, contacts, and deformations simultaneously using a Transformer-based architecture. It features disentangling the regression of local deformation fields and global mesh vertex locations into two network branches, enhancing deformation and contact estimation for precise and robust hand-face mesh recovery. To improve generalizability, we propose a weakly-supervised training approach that augments the training set using in-the-wild images without 3D ground-truth annotations, employing the depths of 2D keypoints estimated by off-the-shelf models and adversarial priors of poses for supervision. Our experiments demonstrate that DICE achieves state-of-the-art performance on a standard benchmark and in-the-wild data in terms of accuracy and physical plausibility. Additionally, our method operates at an interactive rate (20 fps) on an Nvidia 4090 GPU, whereas Decaf requires more than 15 seconds for a single image. Our code will be publicly available upon publication.</p></div></a><div class=btn-links></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/yifan-wu/>Yifan Wu</a></span>, <span><a href=/author/zhiyang-dou/>Zhiyang Dou</a></span>, <span><a href=/author/yuko-ishiwaka/>Yuko Ishiwaka</a></span>, <span><a href=/author/shun-ogawa/>Shun Ogawa</a></span>, <span><a href=/author/yuke-lou/>Yuke Lou</a></span>, <span><a href=/author/wenping-wang/>Wenping Wang</a></span>, <span><a href=/author/lingjie-liu/>Lingjie Liu</a></span>, <span><a href=/author/taku-komura/>Taku Komura</a></span></div><span class=article-date>December 2024</span>
<span class=middot-divider></span><span class=pub-publication>SIGGRAPH Asia 2024</span></div><a href=/publication/cbil-2024/><img src=/publication/cbil-2024/featured_hu64c129ee985796ef8368587d8254d785_2260209_600x400_fit_lanczos_2.png class=article-banner alt="CBIL: Collective Behavior Imitation Learning for Fish from Real Videos"></a><h3 class="article-title mb-1 mt-3"><a href=/publication/cbil-2024/>CBIL: Collective Behavior Imitation Learning for Fish from Real Videos</a></h3><a href=/publication/cbil-2024/ class=summary-link><div class=article-style><p>Reproducing realistic collective behaviors presents a captivating yet formidable challenge. Traditional rule-based methods rely on hand-crafted principles, limiting motion diversity and realism in generated collective behaviors. Recent imitation learning methods learn from data but often require ground truth motion trajectories and struggle with authenticity, especially in high-density groups with erratic movements. In this paper, we present a scalable approach, Collective Behavior Imitation Learning (CBIL), for learning fish schooling behavior directly from videos, without relying on captured motion trajectories. Our method first leverages Video Representation Learning, where a Masked Video AutoEncoder (MVAE) extracts implicit states from video inputs in a self-supervised manner. The MVAE effectively maps 2D observations to implicit states that are compact and expressive for following the imitation learning stage. Then, we propose a novel adversarial imitation learning method to effectively capture complex movements of the schools of fish, allowing for efficient imitation of the distribution for motion patterns measured in the latent space. It also incorporates bio-inspired rewards alongside priors to regularize and stabilize training. Once trained, CBIL can be used for various animation tasks with the learned collective motion priors. We further show its effectiveness across different species. Finally, we demonstrate the application of our system in detecting abnormal fish behavior from in-the-wild videos.</p></div></a><div class=btn-links></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/sinan-wang/>Sinan Wang</a></span>, <span><a href=/author/yitong-deng/>Yitong Deng</a></span>, <span><a href=/author/molin-deng/>Molin Deng</a></span>, <span><a href=/author/hong-xing-yu/>Hong-Xing Yu</a></span>, <span><a href=/author/junwei-zhou/>Junwei Zhou</a></span>, <span><a href=/author/duowen-chen/>Duowen Chen</a></span>, <span><a href=/author/taku-komura/>Taku Komura</a></span>, <span><a href=/author/jiajun-wu/>Jiajun Wu</a></span>, <span><a href=/author/bo-zhu/>Bo Zhu</a></span></div><span class=article-date>December 2024</span>
<span class=middot-divider></span><span class=pub-publication>Siggraph Asia 2024 (ToG)</span></div><a href=/publication/eulerian-2024/><img src=/publication/eulerian-2024/featured_hud6ec3175500424e909fac5a2e2f6c41b_44075_600x400_fit_q90_lanczos.jpg class=article-banner alt="An Eulerian Vortex Method on Flow Maps"></a><h3 class="article-title mb-1 mt-3"><a href=/publication/eulerian-2024/>An Eulerian Vortex Method on Flow Maps</a></h3><a href=/publication/eulerian-2024/ class=summary-link><div class=article-style><p>We present an Eulerian vortex method based on the theory of flow maps to simulate the complex vortical motions of incompressible fluids. Central to our method is the novel incorporation of the flow-map transport equations for line elements, which, in combination with a bi-directional marching scheme for flow maps, enables the high-fidelity Eulerian advection of vorticity variables. The fundamental motivation is that, compared to impulse 𝒎, which has been recently bridged with flow maps to encouraging results, vorticity 𝝎 promises to be preferable for its numerically stability and physical interpretability. To realize the full potential of this novel formulation, we develop a new Poisson solving scheme for vorticity-to-velocity reconstruction that is both efficient and able to accurately handle the coupling near solid boundaries. We demonstrate the efficacy of our approach with a range of vortex simulation examples, including leapfrog vortices, vortex collisions, cavity flow, and the formation of complex vortical structures due to solid-fluid interactions.</p></div></a><div class=btn-links></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/huancheng-lin/>Huancheng Lin</a></span>, <span><a href=/author/floyd-m.-chitalu/>Floyd M. Chitalu</a></span>, <span><a href=/author/taku-komura/>Taku Komura</a></span></div><span class=article-date>August 2024</span>
<span class=middot-divider></span><span class=pub-publication>ACM Transactions on Graphics (TOG)</span></div><a href=/publication/aarap-2024/><img src=/publication/aarap-2024/featured_huaadf189074342e639b8692be34bd271c_364374_600x400_fit_lanczos_2.png class=article-banner alt="Analytic rotation-invariant modelling of anisotropic finite elements"></a><h3 class="article-title mb-1 mt-3"><a href=/publication/aarap-2024/>Analytic rotation-invariant modelling of anisotropic finite elements</a></h3><a href=/publication/aarap-2024/ class=summary-link><div class=article-style><p>Anisotropic hyperelastic distortion energies are used to solve many problems in fields like computer graphics and engineering with applications in shape analysis, deformation, design, mesh parameterization, biomechanics and more. However, formulating a robust anisotropic energy that is low-order and yet sufficiently non-linear remains a challenging problem for achieving the convergence promised by Newton-type methods in numerical optimization. In this paper, we propose a novel analytic formulation of an anisotropic energy that is smooth everywhere, low-order, rotationally-invariant and at-least twice differentiable. At its core, our approach utilizes implicit rotation factorizations with invariants of the Cauchy-Green tensor that arises from the deformation gradient. The versatility and generality of our analysis is demonstrated through a variety of examples, where we also show that the constitutive law suggested by the anisotropic version of the well-known \textit{As-Rigid-As-Possible} energy is the foundational parametric description of both passive and active elastic materials. The generality of our approach means that we can systematically derive the force and force-Jacobian expressions for use in implicit and quasistatic numerical optimization schemes, and we can also use our analysis to rewrite, simplify and speedup several existing anisotropic \textit{and} isotropic distortion energies with guaranteed inversion-safety.</p></div></a><div class=btn-links></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/wenyang-zhou/>Wenyang Zhou</a></span>, <span><a href=/author/zhiyang-dou/>Zhiyang Dou</a></span>, <span><a href=/author/zeyu-cao/>Zeyu Cao</a></span>, <span><a href=/author/zhouyingcheng-liao/>Zhouyingcheng Liao</a></span>, <span><a href=/author/jingbo-wang/>Jingbo Wang</a></span>, <span><a href=/author/wenjia-wang/>Wenjia Wang</a></span>, <span><a href=/author/yuan-liu/>Yuan Liu</a></span>, <span><a href=/author/taku-komura/>Taku Komura</a></span>, <span><a href=/author/wenping-wang/>Wenping Wang</a></span>, <span><a href=/author/lingjie-liu/>Lingjie Liu</a></span></div><span class=article-date>July 2024</span>
<span class=middot-divider></span><span class=pub-publication>ECCV 2024</span></div><a href=/publication/emdm-2024/><img src=/publication/emdm-2024/featured_hube3b2490a0dcb2db5623ef33d10d6870_1275743_600x400_fit_lanczos_2.png class=article-banner alt="EMDM: Efficient Motion Diffusion Model for Fast and High-Quality Motion Generation"></a><h3 class="article-title mb-1 mt-3"><a href=/publication/emdm-2024/>EMDM: Efficient Motion Diffusion Model for Fast and High-Quality Motion Generation</a></h3><a href=/publication/emdm-2024/ class=summary-link><div class=article-style><p>We introduce Efficient Motion Diffusion Model (EMDM) for fast and high-quality human motion generation. Current state-of-the-art generative diffusion models have produced impressive results but struggle to achieve fast generation without sacrificing quality. On the one hand, previous works, like motion latent diffusion, conduct diffusion within a latent space for efficiency, but learning such a latent space can be a non-trivial effort. On the other hand, accelerating generation by naively increasing the sampling step size, e.g., DDIM, often leads to quality degradation as it fails to approximate the complex denoising distribution. To address these issues, we propose EMDM, which captures the complex distribution during multiple sampling steps in the diffusion model, allowing for much fewer sampling steps and significant acceleration in generation. This is achieved by a conditional denoising diffusion GAN to capture multimodal data distributions among arbitrary (and potentially larger) step sizes conditioned on control signals, enabling fewer-step motion sampling with high fidelity and diversity. To minimize undesired motion artifacts, geometric losses are imposed during network learning. As a result, EMDM achieves real-time motion generation and significantly improves the efficiency of motion diffusion models compared to existing methods while achieving high-quality motion generation. Our code is available at \url{https://github.com/Frank-ZY-Dou/EMDM}.</p></div></a><div class=btn-links></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/zhengming-yu/>Zhengming Yu</a></span>, <span><a href=/author/zhiyang-dou/>Zhiyang Dou</a></span>, <span><a href=/author/xiaoxiao-long/>Xiaoxiao Long</a></span>, <span><a href=/author/cheng-lin/>Cheng Lin</a></span>, <span><a href=/author/zekun-li/>Zekun Li</a></span>, <span><a href=/author/yuan-liu/>Yuan Liu</a></span>, <span><a href=/author/norman-muller/>Norman Müller</a></span>, <span><a href=/author/taku-komura/>Taku Komura</a></span>, <span><a href=/author/marc-habermann/>Marc Habermann</a></span>, <span><a href=/author/christian-theobalt/>Christian Theobalt</a></span>, <span><a href=/author/xin-li/>Xin Li</a></span>, <span><a href=/author/wenping-wang/>Wenping Wang</a></span></div><span class=article-date>July 2024</span>
<span class=middot-divider></span><span class=pub-publication>ECCV 2024</span></div><a href=/publication/surfd-2024/><img src=/publication/surfd-2024/featured_hu6de8966873e73a8f70e455b73b95dd73_1532477_600x400_fit_lanczos_2.png class=article-banner alt="Surf-D: Generating High-Quality Surfaces of Arbitrary Topologies Using Diffusion Models"></a><h3 class="article-title mb-1 mt-3"><a href=/publication/surfd-2024/>Surf-D: Generating High-Quality Surfaces of Arbitrary Topologies Using Diffusion Models</a></h3><a href=/publication/surfd-2024/ class=summary-link><div class=article-style><p>We present Surf-D, a novel method for generating high-quality 3D shapes as Surfaces with arbitrary topologies using Diffusion models. Previous methods explored shape generation with different representations and they suffer from limited topologies and poor geometry details. To generate high-quality surfaces of arbitrary topologies, we use the Unsigned Distance Field (UDF) as our surface representation to accommodate arbitrary topologies. Furthermore, we propose a new pipeline that employs a point-based AutoEncoder to learn a compact and continuous latent space for accurately encoding UDF and support high-resolution mesh extraction. We further show that our new pipeline significantly outperforms the prior approaches to learning the distance fields, such as the grid-based AutoEncoder, which is not scalable and incapable of learning accurate UDF. In addition, we adopt a curriculum learning strategy to efficiently embed various surfaces. With the pretrained shape latent space, we employ a latent diffusion model to acquire the distribution of various shapes. Extensive experiments are presented on using Surf-D for unconditional generation, category conditional generation, image conditional generation, and text-to-shape tasks. The experiments demonstrate the superior performance of Surf-D in shape generation across multiple modalities as conditions.</p></div></a><div class=btn-links></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/weilin-wan/>Weilin Wan</a></span>, <span><a href=/author/zhiyang-dou/>Zhiyang Dou</a></span>, <span><a href=/author/taku-komura/>Taku Komura</a></span>, <span><a href=/author/wenping-wang/>Wenping Wang</a></span>, <span><a href=/author/dinesh-jayaraman/>Dinesh Jayaraman</a></span>, <span><a href=/author/lingjie-liu/>Lingjie Liu</a></span></div><span class=article-date>July 2024</span>
<span class=middot-divider></span><span class=pub-publication>ECCV 2024</span></div><a href=/publication/tlcontrol-2024/><img src=/publication/tlcontrol-2024/featured_huf9956d4783a3a6f5efbb958fca43e061_4138965_600x400_fit_lanczos_2.png class=article-banner alt="TLControl: Trajectory and Language Control for Human Motion Synthesis"></a><h3 class="article-title mb-1 mt-3"><a href=/publication/tlcontrol-2024/>TLControl: Trajectory and Language Control for Human Motion Synthesis</a></h3><a href=/publication/tlcontrol-2024/ class=summary-link><div class=article-style><p>Controllable human motion synthesis is essential for applications in AR/VR, gaming, movies, and embodied AI. Existing methods often focus solely on either language or full trajectory control, lacking precision in synthesizing motions aligned with user-specified trajectories, especially for multi-joint control. To address these issues, we present TLControl, a new method for realistic human motion synthesis, incorporating both low-level trajectory and high-level language semantics controls. Specifically, we first train a VQ-VAE to learn a compact latent motion space organized by body parts. We then propose a Masked Trajectories Transformer to make coarse initial predictions of full trajectories of joints based on the learned latent motion space, with user-specified partial trajectories and text descriptions as conditioning. Finally, we introduce an efficient test-time optimization to refine these coarse predictions for accurate trajectory control. Experiments demonstrate that TLControl outperforms the state-of-the-art in trajectory accuracy and time efficiency, making it practical for interactive and high-quality animation generation.</p></div></a><div class=btn-links></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/zhouyingcheng-liao/>Zhouyingcheng Liao</a></span>, <span><a href=/author/sinan-wang/>Sinan Wang</a></span>, <span><a href=/author/taku-komura/>Taku Komura</a></span></div><span class=article-date>July 2024</span>
<span class=middot-divider></span><span class=pub-publication>ECCV 2024</span></div><a href=/publication/senc-2024/><img src=/publication/senc-2024/featured_hub8bf909d1c6bb6e1246b02a8f52b8db2_219071_600x400_fit_lanczos_2.png class=article-banner alt="SENC: Handling Self-collision in Neural Cloth Simulation"></a><h3 class="article-title mb-1 mt-3"><a href=/publication/senc-2024/>SENC: Handling Self-collision in Neural Cloth Simulation</a></h3><a href=/publication/senc-2024/ class=summary-link><div class=article-style><p>We present SENC, a novel self-supervised neural cloth simulator that addresses the challenge of cloth self-collision. This problem has remained unresolved due to the gap in simulation setup between recent collision detection and response approaches and self-supervised neural simulators. The former requires collision-free initial setups, while the latter necessitates random cloth instantiation during training. To tackle this issue, we propose a novel loss based on Global Intersection Analysis (GIA). This loss extracts the volume surrounded by the cloth region that forms the penetration. By constructing an energy based on this volume, our self-supervised neural simulator can effectively address cloth self-collisions. Moreover, we develop a self-collision-aware graph neural network capable of learning to handle self-collisions, even for parts that are topologically distant from one another. Additionally, we introduce an effective external force scheme that enables the simulation to learn the cloth&rsquo;s behavior in response to random external forces. We validate the efficacy of SENC through extensive quantitative and qualitative experiments, demonstrating that it effectively reduces cloth self-collision while maintaining high-quality animation results.</p></div></a><div class=btn-links></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/rui-chen/>Rui Chen</a></span>, <span><a href=/author/mingyi-shi/>Mingyi Shi</a></span>, <span><a href=/author/shaoli-huang/>Shaoli Huang</a></span>, <span><a href=/author/ping-tan/>Ping Tan</a></span>, <span><a href=/author/taku-komura/>Taku Komura</a></span>, <span><a href=/author/xuelin-chen/>Xuelin Chen</a></span></div><span class=article-date>July 2024</span>
<span class=middot-divider></span><span class=pub-publication>SIGGRAPH 2024</span></div><a href=/publication/camdm-2024/><img src=/publication/camdm-2024/featured_hu70c25340799bdb71c86f682a773b0b1d_938985_600x400_fit_lanczos_2.png class=article-banner alt="Taming Diffusion Probabilistic Models for Character Control"></a><h3 class="article-title mb-1 mt-3"><a href=/publication/camdm-2024/>Taming Diffusion Probabilistic Models for Character Control</a></h3><a href=/publication/camdm-2024/ class=summary-link><div class=article-style><p>We present a novel character control framework that effectively utilizes motion diffusion probabilistic models to generate high-quality and diverse character animations, responding in real-time to a variety of dynamic user-supplied control signals. At the heart of our method lies a transformer-based Conditional Autoregressive Motion Diffusion Model (CAMDM), which takes as input the character&rsquo;s historical motion and can generate a range of diverse potential future motions conditioned on high-level, coarse user control. To meet the demands for diversity, controllability, and computational efficiency required by a real-time controller, we incorporate several key algorithmic designs. These include separate condition tokenization, classifier-free guidance on past motion, and heuristic future trajectory extension, all designed to address the challenges associated with taming motion diffusion probabilistic models for character control. As a result, our work represents the first model that enables real-time generation of high-quality, diverse character animations based on user interactive control, supporting animating the character in multiple styles with a single unified model. We evaluate our method on a diverse set of locomotion skills, demonstrating the merits of our method over existing character controllers.</p></div></a><div class=btn-links></div></div><div class=see-all><a href=/publication/>See all publications
<i class="fas fa-angle-right"></i></a></div></div></div></div></section><section id=tags class="home-section wg-tag-cloud"><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Popular Topics</h1></div><div class="col-12 col-lg-8"><div class=tag-cloud><a href=/tag/3d-human-generation/ style=font-size:.7rem>3D human generation</a>
<a href=/tag/3d-human-motion-generation/ style=font-size:1.35rem>3D human motion generation</a>
<a href=/tag/3d-human-reconstruction/ style=font-size:1.7302256254687514rem>3D human reconstruction</a>
<a href=/tag/3d-motion-reconstruction/ style=font-size:.7rem>3D motion reconstruction</a>
<a href=/tag/anisotropy/ style=font-size:.7rem>Anisotropy</a>
<a href=/tag/barrier-hessian/ style=font-size:.7rem>Barrier Hessian</a>
<a href=/tag/character-controller/ style=font-size:.7rem>character controller</a>
<a href=/tag/computer-animation/ style=font-size:.7rem>Computer Animation</a>
<a href=/tag/crowd-animation/ style=font-size:.7rem>Crowd Animation</a>
<a href=/tag/diffusion-model/ style=font-size:1.35rem>diffusion model</a>
<a href=/tag/eigen-analysis/ style=font-size:.7rem>Eigen Analysis</a>
<a href=/tag/facial-animation/ style=font-size:.7rem>Facial Animation</a>
<a href=/tag/facial-rigging/ style=font-size:.7rem>Facial Rigging</a>
<a href=/tag/finite-elements/ style=font-size:.7rem>Finite Elements</a>
<a href=/tag/gail/ style=font-size:1.35rem>GAIL</a>
<a href=/tag/geometric-modeling/ style=font-size:1.35rem>Geometric Modeling</a>
<a href=/tag/medial-axis-transform/ style=font-size:1.35rem>Medial Axis Transform</a>
<a href=/tag/physical-simulation/ style=font-size:1.35rem>Physical Simulation</a>
<a href=/tag/physics-based-character-animation/ style=font-size:1.7302256254687514rem>Physics-based Character Animation</a>
<a href=/tag/rl/ style=font-size:1.7302256254687514rem>RL</a></div></div></div></div></section><section id=contact class="home-section wg-contact"><div class=container><div class="row contact-widget"><div class="col-12 col-lg-4 section-heading"><h1>Contact</h1></div><div class="col-12 col-lg-8"><div class=mb-3><form name=contact method=post action=https://formspree.io/f/xpzojplq><div class="form-group form-inline"><label class=sr-only for=inputName>Name</label>
<input type=text name=name class="form-control w-100" id=inputName placeholder=Name required></div><div class="form-group form-inline"><label class=sr-only for=inputEmail>Email</label>
<input type=email name=email class="form-control w-100" id=inputEmail placeholder=Email required></div><div class=form-group><label class=sr-only for=inputMessage>Message</label>
<textarea name=message class=form-control id=inputMessage rows=5 placeholder=Message required></textarea></div><button type=submit class="btn btn-outline-primary px-3 py-2">Send</button></form></div><ul class=fa-ul><li><i class="fa-li fas fa-map-marker fa-2x" aria-hidden=true></i><span id=person-address>CYC Building, the University of Hong Kong, Hong Kong,</span></li></ul><div class=d-none><input id=map-provider value=1>
<input id=map-lat value=22.2832>
<input id=map-lng value=114.1354>
<input id=map-dir value="CYC Building, the University of Hong Kong, Hong Kong,">
<input id=map-zoom value=14>
<input id=map-api-key value=AIzaSyDRIRPRmqFRMGZ0_d60XJceM3MMWhGS1NQ></div><div id=map></div></div></div></div></section><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js></script><script async defer src="https://maps.googleapis.com/maps/api/js?key=AIzaSyDRIRPRmqFRMGZ0_d60XJceM3MMWhGS1NQ%20%20"></script><script src=https://cdnjs.cloudflare.com/ajax/libs/gmaps.js/0.4.25/gmaps.min.js integrity="sha256-7vjlAeb8OaTrCXZkCNun9djzuB2owUsaO72kXaFDBJs=" crossorigin=anonymous></script><script>const code_highlighting=true;</script><script>const search_config={"indexURI":"/index.json","minLength":1,"threshold":0.3};const i18n={"no_results":"No results found","placeholder":"Search...","results":"results found"};const content_type={'post':"Posts",'project':"Projects",'publication':"Publications",'talk':"Talks",'slides':"Slides"};</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script>if(window.netlifyIdentity){window.netlifyIdentity.on("init",user=>{if(!user){window.netlifyIdentity.on("login",()=>{document.location.href="/admin/";});}});}</script><script src=/js/wowchemy.min.4c2bca31150ce93c5a5e43b8a50f22fd.js></script><div class=container><footer class=site-footer><p class=powered-by></p><p class=powered-by>Published with
<a href=https://wowchemy.com target=_blank rel=noopener>Wowchemy</a> —
the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.
<span class=float-right aria-hidden=true><a href=# class=back-to-top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body></html>